{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Blaine Hill\n",
    "\n",
    "In this notebook, we train a score matching model as described by [Yang Song](https://yang-song.net/blog/2021/score/)\n",
    "\n",
    "\n",
    "$$s_\\theta(x) = \\nabla_x \\log p_\\theta(x) = -\\nabla_x f_\\theta(x) - \\nabla_x \\log Z_\\theta = -\\nabla_x f_\\theta(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blaineh2/anaconda3/envs/ScoreMatchingDiffKG/lib/python3.9/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1\n",
      "Train Loss: 21.82866895995045908307\n",
      "Test Loss: 12.62060875579714824823\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 221\u001b[0m\n\u001b[1;32m    211\u001b[0m     compute_metrics(score_model, test_loader, config)\n\u001b[1;32m    213\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: device,\n\u001b[1;32m    219\u001b[0m }\n\u001b[0;32m--> 221\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 211\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    208\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m test_model(score_model, test_loader, config)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.20f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.20f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 211\u001b[0m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 176\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(score_model, test_loader, config)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(r_idx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    175\u001b[0m     r_init_emb \u001b[38;5;241m=\u001b[39m score_model\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;241m.\u001b[39mrel_emb(candidate_r_idx[i])\n\u001b[0;32m--> 176\u001b[0m     refined_candidate \u001b[38;5;241m=\u001b[39m \u001b[43mreverse_sde\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_emb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_init_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     refined_candidates\u001b[38;5;241m.\u001b[39mappend(refined_candidate)\n\u001b[1;32m    178\u001b[0m refined_candidates \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(refined_candidates, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 72\u001b[0m, in \u001b[0;36mreverse_sde\u001b[0;34m(h, r_init, t, score_model, config)\u001b[0m\n\u001b[1;32m     70\u001b[0m t_steps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     71\u001b[0m score_sde_func \u001b[38;5;241m=\u001b[39m sde_func_wrapper(h, t, score_model)\n\u001b[0;32m---> 72\u001b[0m r_emb \u001b[38;5;241m=\u001b[39m \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_sde_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_steps\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r_emb\n",
      "File \u001b[0;32m~/anaconda3/envs/ScoreMatchingDiffKG/lib/python3.9/site-packages/torchdiffeq/_impl/odeint.py:77\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     74\u001b[0m solver \u001b[38;5;241m=\u001b[39m SOLVERS[method](func\u001b[38;5;241m=\u001b[39mfunc, y0\u001b[38;5;241m=\u001b[39my0, rtol\u001b[38;5;241m=\u001b[39mrtol, atol\u001b[38;5;241m=\u001b[39matol, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     solution \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     event_t, solution \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39mintegrate_until_event(t[\u001b[38;5;241m0\u001b[39m], event_fn)\n",
      "File \u001b[0;32m~/anaconda3/envs/ScoreMatchingDiffKG/lib/python3.9/site-packages/torchdiffeq/_impl/solvers.py:28\u001b[0m, in \u001b[0;36mAdaptiveStepsizeODESolver.integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     26\u001b[0m solution[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my0\n\u001b[1;32m     27\u001b[0m t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_before_integrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(t)):\n\u001b[1;32m     30\u001b[0m     solution[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_advance(t[i])\n",
      "File \u001b[0;32m~/anaconda3/envs/ScoreMatchingDiffKG/lib/python3.9/site-packages/torchdiffeq/_impl/rk_common.py:161\u001b[0m, in \u001b[0;36mRKAdaptiveStepsizeODESolver._before_integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_before_integrate\u001b[39m(\u001b[38;5;28mself\u001b[39m, t):\n\u001b[1;32m    160\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m t[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 161\u001b[0m     f0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m         first_step \u001b[38;5;241m=\u001b[39m _select_initial_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, t[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my0, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morder \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrtol, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matol,\n\u001b[1;32m    164\u001b[0m                                           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm, f0\u001b[38;5;241m=\u001b[39mf0)\n",
      "File \u001b[0;32m~/anaconda3/envs/ScoreMatchingDiffKG/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ScoreMatchingDiffKG/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ScoreMatchingDiffKG/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py:189\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 62\u001b[0m, in \u001b[0;36msde_func_wrapper.<locals>.score_sde\u001b[0;34m(time, r_emb)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mFunction for the reverse SDE process.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 62\u001b[0m     score_grad \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mr_emb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mscore_grad\n",
      "File \u001b[0;32m~/anaconda3/envs/ScoreMatchingDiffKG/lib/python3.9/site-packages/torch/autograd/__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    408\u001b[0m         grad_outputs_\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    424\u001b[0m     ):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.nn import RotatE\n",
    "from torch_geometric.datasets import FB15k_237\n",
    "import sys\n",
    "import os\n",
    "import os.path as osp\n",
    "import math\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "from torchdiffeq import odeint \n",
    "\n",
    "parent_dir = os.path.abspath(os.pardir)\n",
    "sys.path.append(parent_dir)\n",
    "data_path = osp.join(parent_dir, \"data\", \"FB15k_237\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_data = FB15k_237(data_path, split=\"train\")[0].to(device)\n",
    "val_data = FB15k_237(data_path, split=\"val\")[0].to(device)\n",
    "test_data = FB15k_237(data_path, split=\"test\")[0].to(device)\n",
    "\n",
    "\n",
    "class ScoreModel(nn.Module):\n",
    "    def __init__(self, embedding_model, node_emb_dim, rel_emb_dim, config):\n",
    "        super(ScoreModel, self).__init__()\n",
    "        self.config = config  # store config for hyperparameters\n",
    "        self.embedding_model = embedding_model\n",
    "        self.node_emb_dim = node_emb_dim\n",
    "        self.rel_emb_dim = rel_emb_dim\n",
    "        self.score_net = nn.Sequential(\n",
    "            nn.Linear(node_emb_dim + rel_emb_dim, 512, dtype=torch.float),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512, dtype=torch.float),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1, dtype=torch.float),\n",
    "        )\n",
    "\n",
    "    def forward(self, h_emb, r_emb, t_emb, timestep=None):\n",
    "        # Implement your desired distance measure here (e.g., L2 distance)\n",
    "        distance = torch.linalg.norm(h_emb + r_emb - t_emb, dim=-1)\n",
    "\n",
    "        # Gradually increase the weight of the distance term during SDE steps\n",
    "        weight = 0.0  # No weight increase if timestep is None\n",
    "        if timestep is not None:\n",
    "            def sigmoid(x):\n",
    "                return 1 / (1 + math.exp(-x))\n",
    "            weight = sigmoid(timestep / (self.config[\"num_steps\"] - 1))\n",
    "        score = weight * distance\n",
    "        return score\n",
    "\n",
    "\n",
    "def build_model(config):\n",
    "    embedding_model = RotatE(num_nodes=train_data.num_nodes, num_relations=train_data.num_edge_types, hidden_channels=50).to(config[\"device\"])\n",
    "    embedding_model.load_state_dict(torch.load(\"../embedding_model/FB15k_237_RotatE_embedding_model_weights.pth\"))\n",
    "    embedding_model.eval()\n",
    "\n",
    "    # Get entity and relation embeddings\n",
    "    with torch.no_grad():\n",
    "        entity_embeddings_real = embedding_model.node_emb.weight.detach()\n",
    "        entity_embeddings_im = embedding_model.node_emb_im.weight.detach()\n",
    "        entity_embeddings = torch.cat([entity_embeddings_real, entity_embeddings_im], dim=-1)\n",
    "        relation_embeddings = embedding_model.rel_emb.weight.detach()\n",
    "\n",
    "    score_model = ScoreModel(embedding_model, entity_embeddings.shape[-1], relation_embeddings.shape[-1], config).to(config[\"device\"])\n",
    "\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(train_data.edge_index[0], train_data.edge_type, train_data.edge_index[1])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(val_data.edge_index[0], val_data.edge_type, val_data.edge_index[1])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    test_dataset = TensorDataset(test_data.edge_index[0], test_data.edge_type, test_data.edge_index[1])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    optimizer = Adam(score_model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "\n",
    "    return score_model, optimizer, train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def denoising_score_matching_loss(score_model, h, r, t, timestep):\n",
    "    \"\"\"\n",
    "    Denoising score-matching loss with noise-conditional score networks.\n",
    "    \"\"\"\n",
    "    h_emb, r_emb, t_emb = score_model.embedding_model.node_emb(h), score_model.embedding_model.rel_emb(r), score_model.embedding_model.node_emb(t)\n",
    "    true_score = score_model(h_emb, r_emb, t_emb)  # simply do not pass in the timestep\n",
    "    noisy_score = score_model(h_emb, r_emb, t_emb, timestep)\n",
    "    return ((true_score - noisy_score) ** 2).mean()\n",
    "\n",
    "\n",
    "def train_model(score_model, optimizer, train_loader, config):\n",
    "    score_model.train()\n",
    "    loss_epoch = 0\n",
    "    for batch in train_loader:\n",
    "        h, r, t = batch\n",
    "        h, r, t = h.to(config[\"device\"]), r.to(config[\"device\"]), t.to(config[\"device\"])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for timestep in range(config[\"num_steps\"]):\n",
    "            loss = denoising_score_matching_loss(score_model, h, r, t, timestep)\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "    return loss_epoch / len(train_loader)\n",
    "\n",
    "\n",
    "\n",
    "def test_model(score_model, test_loader, config):\n",
    "    score_model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            h, r, t = batch\n",
    "            h, r, t = h.to(config[\"device\"]), r.to(config[\"device\"]), t.to(config[\"device\"])\n",
    "            for timestep in range(config[\"num_steps\"]):\n",
    "                loss = denoising_score_matching_loss(score_model, h, r, t, timestep)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "    return test_loss / (len(test_loader) * config[\"num_steps\"])  # Average across steps\n",
    "\n",
    "def sde_func_wrapper(h, t, score_model):\n",
    "    def score_sde(time, r_emb):\n",
    "        \"\"\"\n",
    "        Function for the reverse SDE process.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            score_grad = torch.autograd.grad(score_model(h, r_emb, t), [r_emb])[0]\n",
    "        return -score_grad  # Update in the direction that minimizes the score\n",
    "    return score_sde\n",
    "\n",
    "def reverse_sde(h, r_init, t, score_model, config):\n",
    "    \"\"\"\n",
    "    Perform reverse SDE to refine the target entity embedding.\n",
    "    \"\"\"\n",
    "    t_steps = torch.linspace(0, 1.0, config[\"num_steps\"]).to(config[\"device\"])\n",
    "    score_sde_func = sde_func_wrapper(h, t, score_model)\n",
    "    r_emb = torch.tensor(r_init, requires_grad=True).to(config[\"device\"])\n",
    "    r_emb = odeint(score_sde_func, r_init, t_steps)[-1, :]\n",
    "    return r_emb\n",
    "\n",
    "def compute_metrics(score_model, test_loader, config):\n",
    "    \"\"\"\n",
    "    Compute link prediction metrics: mean rank, mean reciprocal rank, and Hits@k.\n",
    "    \"\"\"\n",
    "    score_model.eval()\n",
    "    mean_rank = 0.0\n",
    "    mean_reciprocal_rank = 0.0\n",
    "    hits_at_10 = 0\n",
    "    hits_at_3 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            h_idx, r_idx, t_idx = batch\n",
    "            h_idx, t_idx = h_idx.to(config[\"device\"]), t_idx.to(config[\"device\"])\n",
    "            r_idx = r_idx.to(config[\"device\"])\n",
    "\n",
    "            h_emb, r_emb, t_emb = score_model.embedding_model.node_emb(h_idx), score_model.embedding_model.rel_emb(r_idx), score_model.embedding_model.node_emb(t_idx)\n",
    "\n",
    "            # Sample a set of candidate target entities (r')\n",
    "            num_candidates = 50\n",
    "            candidate_r_idx = torch.randint(0, score_model.rel_emb_dim, size=(r_idx.shape[0], num_candidates)).to(config[\"device\"])\n",
    "\n",
    "            # Refine candidate embeddings using reverse SDE\n",
    "            refined_candidates = []\n",
    "            for i in range(r_idx.shape[0]):\n",
    "                r_init_emb = score_model.embedding_model.rel_emb(candidate_r_idx[i])\n",
    "                refined_candidate = reverse_sde(h_emb[i], r_init_emb, t_emb[i], score_model, config)\n",
    "                refined_candidates.append(refined_candidate)\n",
    "            refined_candidates = torch.stack(refined_candidates, dim=0)\n",
    "\n",
    "            # Compute scores for all candidates (including ground truth)\n",
    "            # all_r = torch.cat([r_emb.unsqueeze(1), refined_candidates], dim=1)\n",
    "            scores = score_model(h_emb.repeat(1, num_candidates + 1), r_emb.repeat(1, num_candidates + 1), t_emb.repeat(1, num_candidates + 1), torch.ones(r_idx.shape[0], num_candidates + 1) * config[\"num_steps\"]).squeeze(2)\n",
    "\n",
    "            # Evaluate link prediction metrics\n",
    "            for i in range(r_idx.shape[0]):\n",
    "                ground_truth_rank = (scores[i] == scores[i].max()).nonzero(as_tuple=True)[0].item()\n",
    "                mean_rank += 1 + ground_truth_rank\n",
    "                mean_reciprocal_rank += 1.0 / (1 + ground_truth_rank)\n",
    "                hits_at_10 += (scores[i][:10] == scores[i].max()).any().item()\n",
    "                hits_at_3 += (scores[i][:3] == scores[i].max()).any().item()\n",
    "\n",
    "    mean_rank /= len(test_loader.dataset)\n",
    "    mean_reciprocal_rank /= len(test_loader.dataset)\n",
    "    hits_at_10 /= len(test_loader.dataset)\n",
    "    hits_at_3 /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"\\nMean Rank: {mean_rank:.4f}\")\n",
    "    print(f\"Mean Reciprocal Rank: {mean_reciprocal_rank:.4f}\")\n",
    "    print(f\"Hits@10: {hits_at_10:.4f}\")\n",
    "    print(f\"Hits@3: {hits_at_3:.4f}\")\n",
    "\n",
    "\n",
    "def main(config):\n",
    "    score_model, optimizer, train_loader, val_loader, test_loader = build_model(config)\n",
    "    \n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train_model(score_model, optimizer, train_loader, config)\n",
    "        test_loss = test_model(score_model, test_loader, config)\n",
    "        print(f\"\\n\\nEpoch {epoch + 1}\\nTrain Loss: {train_loss:.20f}\\nTest Loss: {test_loss:.20f}\")\n",
    "\n",
    "    compute_metrics(score_model, test_loader, config)\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 1,\n",
    "    \"num_steps\": 20,  # Adjust based on your desired SDE iterations\n",
    "    \"lr\": 1e-4,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "main(config)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ScoreMatchingDiffKG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
