{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Blaine Hill\n",
    "\n",
    "In this notebook, we train a score matching model as described by [Yang Song](https://yang-song.net/blog/2021/score/)\n",
    "\n",
    "\n",
    "$$s_\\theta(x) = \\nabla_x \\log p_\\theta(x) = -\\nabla_x f_\\theta(x) - \\nabla_x \\log Z_\\theta = -\\nabla_x f_\\theta(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.nn import RotatE\n",
    "from torch_geometric.datasets import FB15k_237\n",
    "import sys\n",
    "import os\n",
    "import os.path as osp\n",
    "import math\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "from torchdiffeq import odeint \n",
    "\n",
    "parent_dir = os.path.abspath(os.pardir)\n",
    "sys.path.append(parent_dir)\n",
    "data_path = osp.join(parent_dir, \"data\", \"FB15k_237\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_data = FB15k_237(data_path, split=\"train\")[0].to(device)\n",
    "val_data = FB15k_237(data_path, split=\"val\")[0].to(device)\n",
    "test_data = FB15k_237(data_path, split=\"test\")[0].to(device)\n",
    "\n",
    "\n",
    "class ScoreModel(nn.Module):\n",
    "    def __init__(self, embedding_model, embedding_dim, relation_dim, config):\n",
    "        super(ScoreModel, self).__init__()\n",
    "        self.embedding_model = embedding_model\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.relation_dim = relation_dim\n",
    "        self.config = config \n",
    "\n",
    "        self.score_net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + relation_dim, 512, dtype=torch.float),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512, dtype=torch.float),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1, dtype=torch.float),\n",
    "        )\n",
    "\n",
    "    def forward(self, h, r, t, timestep=None):\n",
    "        h_emb = self.embedding_model.node_emb(h)\n",
    "        r_emb = self.embedding_model.rel_emb(r)\n",
    "        t_emb = self.embedding_model.node_emb(t)\n",
    "\n",
    "        # Implement your desired distance measure here (e.g., L2 distance)\n",
    "        distance = torch.linalg.norm(h_emb + r_emb - t_emb, dim=-1)\n",
    "\n",
    "        # Gradually increase the weight of the distance term during SDE steps\n",
    "        weight = 0.0  # No weight increase if timestep is None\n",
    "        if timestep is not None:\n",
    "            def sigmoid(x):\n",
    "                return 1 / (1 + math.exp(-x))\n",
    "            weight = sigmoid(timestep / (self.config[\"num_steps\"] - 1))\n",
    "        score = weight * distance\n",
    "        return score\n",
    "\n",
    "\n",
    "\n",
    "def sde_func_wrapper(h, t, score_model):\n",
    "    def score_sde(time, r_emb):\n",
    "        \"\"\"\n",
    "        Function for the reverse SDE process.\n",
    "        \"\"\"\n",
    "        ic(r_emb)\n",
    "        ic(r_emb.shape)\n",
    "        with torch.no_grad():\n",
    "            score_grad = torch.autograd.grad(score_model(h, r_emb, t), [r_emb])[0]\n",
    "        return -score_grad  # Update in the direction that minimizes the score\n",
    "    return score_sde\n",
    "\n",
    "def reverse_sde(h, r_init, t, score_model, config):\n",
    "    \"\"\"\n",
    "    Perform reverse SDE to refine the target entity embedding.\n",
    "    \"\"\"\n",
    "    t_steps = torch.linspace(0, 1.0, config[\"num_steps\"]).to(config[\"device\"])\n",
    "    score_sde_func = sde_func_wrapper(h, t, score_model)\n",
    "    r_emb = odeint(score_sde_func, r_init, t_steps)[-1, :]\n",
    "    return r_emb\n",
    "\n",
    "\n",
    "def build_model(config):\n",
    "    embedding_model = RotatE(num_nodes=train_data.num_nodes, num_relations=train_data.num_edge_types, hidden_channels=50).to(config[\"device\"])\n",
    "    embedding_model.load_state_dict(torch.load(\"../embedding_model/FB15k_237_RotatE_embedding_model_weights.pth\"))\n",
    "    embedding_model.eval()\n",
    "\n",
    "    # Get entity and relation embeddings\n",
    "    with torch.no_grad():\n",
    "        entity_embeddings_real = embedding_model.node_emb.weight.detach()\n",
    "        entity_embeddings_im = embedding_model.node_emb_im.weight.detach()\n",
    "        entity_embeddings = torch.cat([entity_embeddings_real, entity_embeddings_im], dim=-1)\n",
    "        relation_embeddings = embedding_model.rel_emb.weight.detach()\n",
    "\n",
    "    score_model = ScoreModel(embedding_model, entity_embeddings.shape[-1], relation_embeddings.shape[-1], config).to(config[\"device\"])\n",
    "\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(train_data.edge_index[0], train_data.edge_type, train_data.edge_index[1])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(val_data.edge_index[0], val_data.edge_type, val_data.edge_index[1])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    test_dataset = TensorDataset(test_data.edge_index[0], test_data.edge_type, test_data.edge_index[1])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    optimizer = Adam(score_model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "\n",
    "    return score_model, optimizer, train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def denoising_score_matching_loss(score_model, h, r, t, timestep):\n",
    "    \"\"\"\n",
    "    Denoising score-matching loss with noise-conditional score networks.\n",
    "    \"\"\"\n",
    "    true_score = score_model(h, r, t)  # simply do not pass in the timestep\n",
    "    noisy_score = score_model(h, r, t, timestep)\n",
    "    return ((true_score - noisy_score) ** 2).mean()\n",
    "\n",
    "\n",
    "def train_model(score_model, optimizer, train_loader, config):\n",
    "    score_model.train()\n",
    "    loss_epoch = 0\n",
    "    for batch in train_loader:\n",
    "        h, r, t = batch\n",
    "        h, r, t = h.to(config[\"device\"]), r.to(config[\"device\"]), t.to(config[\"device\"])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for timestep in range(config[\"num_steps\"]):\n",
    "            loss = denoising_score_matching_loss(score_model, h, r, t, timestep)\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "    return loss_epoch / len(train_loader)\n",
    "\n",
    "\n",
    "\n",
    "def test_model(score_model, test_loader, config):\n",
    "    score_model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            h, r, t = batch\n",
    "            h, r, t = h.to(config[\"device\"]), r.to(config[\"device\"]), t.to(config[\"device\"])\n",
    "            for timestep in range(config[\"num_steps\"]):\n",
    "                loss = denoising_score_matching_loss(score_model, h, r, t, timestep)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "    return test_loss / (len(test_loader) * config[\"num_steps\"])  # Average across steps\n",
    "\n",
    "\n",
    "def compute_metrics(score_model, test_loader, config):\n",
    "    \"\"\"\n",
    "    Compute link prediction metrics: mean rank, mean reciprocal rank, and Hits@k.\n",
    "    \"\"\"\n",
    "    score_model.eval()\n",
    "    mean_rank = 0.0\n",
    "    mean_reciprocal_rank = 0.0\n",
    "    hits_at_10 = 0\n",
    "    hits_at_3 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            h, r, t = batch\n",
    "            h, t = h.to(config[\"device\"]), t.to(config[\"device\"])\n",
    "\n",
    "            # Sample a set of candidate target entities (r')\n",
    "            num_candidates = 50  # You can adjust this value\n",
    "            candidate_r = torch.randint(0, score_model.embedding_model.num_relations, size=(r.shape[0], num_candidates)).to(config[\"device\"])\n",
    "\n",
    "            # Refine candidate embeddings using reverse SDE\n",
    "            refined_candidates = []\n",
    "            for i in range(r.shape[0]):\n",
    "                refined_candidate = reverse_sde(h[i], candidate_r, t[i], score_model, config)\n",
    "                refined_candidates.append(refined_candidate)\n",
    "            refined_candidates = torch.stack(refined_candidates, dim=0)\n",
    "\n",
    "            # Compute scores for all candidates (including ground truth)\n",
    "            all_r = torch.cat([r.unsqueeze(1), refined_candidates], dim=1)\n",
    "            scores = score_model(r.repeat(1, num_candidates + 1), r.repeat(1, num_candidates + 1), all_r, torch.ones(r.shape[0], num_candidates + 1) * config[\"num_steps\"]).squeeze(2)\n",
    "\n",
    "            # Evaluate link prediction metrics\n",
    "            for i in range(h.shape[0]):\n",
    "                ground_truth_rank = (scores[i] == scores[i].max()).nonzero(as_tuple=True)[0].item()\n",
    "                mean_rank += 1 + ground_truth_rank\n",
    "                mean_reciprocal_rank += 1.0 / (1 + ground_truth_rank)\n",
    "                hits_at_10 += (scores[i][:10] == scores[i].max()).any().item()\n",
    "                hits_at_3 += (scores[i][:3] == scores[i].max()).any().item()\n",
    "\n",
    "    mean_rank /= len(test_loader.dataset)\n",
    "    mean_reciprocal_rank /= len(test_loader.dataset)\n",
    "    hits_at_10 /= len(test_loader.dataset)\n",
    "    hits_at_3 /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"\\nMean Rank: {mean_rank:.4f}\")\n",
    "    print(f\"Mean Reciprocal Rank: {mean_reciprocal_rank:.4f}\")\n",
    "    print(f\"Hits@10: {hits_at_10:.4f}\")\n",
    "    print(f\"Hits@3: {hits_at_3:.4f}\")\n",
    "\n",
    "\n",
    "def main(config):\n",
    "    score_model, optimizer, train_loader, val_loader, test_loader = build_model(config)\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train_model(score_model, optimizer, train_loader, config)\n",
    "        test_loss = test_model(score_model, test_loader, config)\n",
    "        print(f\"\\n\\nEpoch {epoch + 1}\\nTrain Loss: {train_loss:.20f}\\nTest Loss: {test_loss:.20f}\")\n",
    "\n",
    "    compute_metrics(score_model, test_loader, config)\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 1,\n",
    "    \"num_steps\": 20,  # Adjust based on your desired SDE iterations\n",
    "    \"lr\": 1e-4,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "main(config)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ScoreMatchingDiffKG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
