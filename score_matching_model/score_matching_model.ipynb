{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Blaine Hill\n",
    "\n",
    "In this notebook, we train a score matching model as described by [Yang Song](https://yang-song.net/blog/2021/score/) to do KG Reasoning tasks such as [link prediction](https://paperswithcode.com/task/link-prediction)\n",
    "\n",
    "$$s_\\theta(x) = \\nabla_x \\log p_\\theta(x) = -\\nabla_x f_\\theta(x) - \\nabla_x \\log Z_\\theta = -\\nabla_x f_\\theta(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"score_matching_model.ipynb\"\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "# wandb.login(relogin=True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import sys\n",
    "import os.path as osp\n",
    "from ipykernel import get_connection_file\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "notebook_path = osp.abspath(\n",
    "    osp.join(os.getcwd(), osp.basename(get_connection_file()))\n",
    ")\n",
    "parent_dir = osp.dirname(osp.dirname(notebook_path))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Import utility functions and model utilities\n",
    "from utils.utils import *\n",
    "from utils.embedding_model_utils import load_dataset\n",
    "from utils.score_matching_model_utils import *\n",
    "\n",
    "# from utils.score_matching_model.utils import *\n",
    "\n",
    "from utils.score_matching_model.ScoreModel import ScoreModel\n",
    "\n",
    "set_seed()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sweep = False\n",
    "sweep_config_file_path = \"sweep_config.yaml\"\n",
    "\n",
    "if run_sweep:\n",
    "    with open(sweep_config_file_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "else:\n",
    "    config = {\n",
    "        \"embedding_model_dir\": \"../trained_embedding_models/Pubmed_TransE_node_classification_2024.05.15.11.58.29_70244147_embedding_model/\",\n",
    "        \"score_model_hidden_dim\": 512,\n",
    "        \"batch_size\": 512,\n",
    "        \"max_epochs\": 3,\n",
    "        \"num_sde_timesteps\": 20,  # Adjust based on your desired SDE iterations\n",
    "        \"lr\": 1e-4,\n",
    "        \"similarity_metric\": \"cosine\",\n",
    "        \"k\": [1, 3, 10],  # used for top-k evaluation\n",
    "        \"verbose\": True,\n",
    "        \"num_epochs_without_improvement_until_early_finish\": 5,\n",
    "        \"validate_after_this_many_epochs\": 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config, view_english=False):\n",
    "    embedding_model_dir = config[\"embedding_model_dir\"]\n",
    "    embedding_model, embedding_model_name, dataset_name = (\n",
    "        initialize_trained_embedding_model(embedding_model_dir, device)\n",
    "    )\n",
    "    embedding_model.eval()\n",
    "\n",
    "    train_data, val_data, test_data, data_path = load_dataset(\n",
    "        dataset_name, parent_dir=parent_dir\n",
    "    )\n",
    "\n",
    "    original_model = ScoreModel(\n",
    "        embedding_model,\n",
    "        config[\"score_model_hidden_dim\"],\n",
    "        config[\"num_sde_timesteps\"],\n",
    "        config[\"similarity_metric\"],\n",
    "        config[\"task\"],\n",
    "        config[\"aux_dict\"],\n",
    "    ).to(device)\n",
    "\n",
    "    # Calculate batch size based on the number of GPUs available\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    batch_size_per_gpu = config[\"batch_size\"]\n",
    "    batch_size = (\n",
    "        batch_size_per_gpu * num_gpus if num_gpus > 0 else batch_size_per_gpu\n",
    "    )\n",
    "    num_workers = num_gpus if num_gpus > 0 else 1\n",
    "    config[\"num_gpus\"] = num_gpus\n",
    "\n",
    "    # Helper function to prepare loader arguments\n",
    "    def prepare_loader_args(data, batch_size, num_workers):\n",
    "        edge_index_0 = data.edge_index[0]\n",
    "        edge_type = data.edge_type\n",
    "        edge_index_1 = data.edge_index[1]\n",
    "\n",
    "        x = data.x if \"x\" in data else None\n",
    "\n",
    "        if x is not None:\n",
    "            dataset = TensorDataset(edge_index_0, edge_type, edge_index_1, x)\n",
    "        else:\n",
    "            dataset = TensorDataset(edge_index_0, edge_type, edge_index_1)\n",
    "\n",
    "        loader_args = {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"shuffle\": True,\n",
    "            \"num_workers\": num_workers,\n",
    "        }\n",
    "\n",
    "        return dataset, loader_args\n",
    "\n",
    "    # Prepare loader arguments for train, validation, and test datasets\n",
    "    train_dataset, train_loader_args = prepare_loader_args(\n",
    "        train_data, batch_size, num_workers\n",
    "    )\n",
    "    val_dataset, val_loader_args = prepare_loader_args(\n",
    "        val_data, batch_size, num_workers\n",
    "    )\n",
    "    test_dataset, test_loader_args = prepare_loader_args(\n",
    "        test_data, batch_size, num_workers\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, **train_loader_args)\n",
    "    val_loader = DataLoader(val_dataset, **val_loader_args)\n",
    "    test_loader = DataLoader(test_dataset, **test_loader_args)\n",
    "\n",
    "    optimizer = Adam(original_model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # Use DataParallel for multi-GPU setups on single server\n",
    "    if num_gpus > 1:\n",
    "        score_model = torch.nn.DataParallel(original_model)\n",
    "    else:\n",
    "        score_model = original_model\n",
    "\n",
    "    # Use DataParallel for multi-GPU setups on single server\n",
    "    if num_gpus > 1:\n",
    "        score_model = torch.nn.DataParallel(original_model)\n",
    "    else:\n",
    "        score_model = original_model\n",
    "\n",
    "    if view_english:\n",
    "        entity_dict, relation_dict = load_dicts(\n",
    "            data_path\n",
    "        )  # note: if you want to load english for RotatE, store the dicts to map from indices to freebase ids/ relations in data_path/processed\n",
    "        score_model.entity_dict = entity_dict\n",
    "        score_model.relation_dict = relation_dict\n",
    "\n",
    "    score_model.train_data = train_data\n",
    "    score_model.val_data = val_data\n",
    "    score_model.test_data = test_data\n",
    "    score_model.train_loader = train_loader\n",
    "    score_model.val_loader = val_loader\n",
    "    score_model.test_loader = test_loader\n",
    "    score_model.optimizer = optimizer\n",
    "    score_model.config = config\n",
    "    model.batch_size = batch_size  # this is different from config batch_size when using multiple GPUs\n",
    "    score_model.dataset_name = dataset_name\n",
    "    score_model.embedding_model_name = embedding_model_name\n",
    "    score_model.embedding_model_dir = embedding_model_dir\n",
    "    score_model.original_model = (\n",
    "        original_model  # Keep a reference to the original model\n",
    "    )\n",
    "\n",
    "    # Determine the path to save results and create directory if it does not exist\n",
    "    if \"save_path\" in config:\n",
    "        save_path = config[\"save_path\"]\n",
    "        if not os.path.exists(save_path):\n",
    "            raise ValueError(\n",
    "                f\"Directory {save_path} does not exist in which to save the trained score matching models. Please create it before saving.\"\n",
    "            )\n",
    "        save_path = osp.join(\n",
    "            save_path,\n",
    "            f\"{config['prefix']}_score_matching_model\",\n",
    "        )\n",
    "    else:\n",
    "        save_path = osp.join(\n",
    "            parent_dir,\n",
    "            \"trained_score_matching_models\",\n",
    "            f\"{config['prefix']}_score_matching_model\",\n",
    "        )\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    score_model.save_path = save_path\n",
    "\n",
    "    # Save the model configuration\n",
    "    save_score_matching_model_config(score_model)\n",
    "\n",
    "    return score_model\n",
    "\n",
    "\n",
    "def train_model(score_model):\n",
    "    score_model.train()\n",
    "    loss_epoch = 0\n",
    "    for batch in tqdm(score_model.train_loader, leave=True):\n",
    "        # Extract data from the batch\n",
    "        h, r, t = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "\n",
    "        # Check if 'x' is present in the batch\n",
    "        x = batch[3].to(device) if len(batch) > 3 else None\n",
    "\n",
    "        for timestep in range(score_model.config[\"num_sde_timesteps\"]):\n",
    "            loss = score_model.original_model.loss(\n",
    "                h, r, t, timestep, x, score_model.config[\"task\"]\n",
    "            )\n",
    "            loss.backward()\n",
    "        score_model.optimizer.step()\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "    return loss_epoch / len(score_model.train_loader)\n",
    "\n",
    "\n",
    "def test_model(score_model, val=False, view_english=False):\n",
    "    score_model.eval()\n",
    "    test_loss = 0\n",
    "    loader = score_model.val_loader if val else score_model.test_loader\n",
    "    all_metrics = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # Extract data from the batch\n",
    "            h, r, t = (\n",
    "                batch[\"head_index\"],\n",
    "                batch[\"rel_type\"],\n",
    "                batch[\"tail_index\"],\n",
    "            )\n",
    "            x = batch.get(\"x\", None)\n",
    "\n",
    "            # Move data to the appropriate device\n",
    "            h, r, t = (\n",
    "                h.to(device),\n",
    "                r.to(device),\n",
    "                t.to(device),\n",
    "            )\n",
    "            if x is not None:\n",
    "                x = x.to(device)\n",
    "\n",
    "            for timestep in range(score_model.config[\"num_sde_timesteps\"]):\n",
    "                loss = score_model.original_model.loss(\n",
    "                    score_model, h, r, t, timestep\n",
    "                )\n",
    "                test_loss += loss.item()\n",
    "\n",
    "            # Calculate metrics for the batch\n",
    "            metrics = model.original_model.test(\n",
    "                h=h,\n",
    "                r=r,\n",
    "                t=t,\n",
    "                x=x,\n",
    "                k=model.config[\"k\"],\n",
    "                task=model.original_model.config[\"task\"],\n",
    "                only_relation_prediction=only_relation_prediction,  # for kg_completion: during validation, only evaluate on relation prediction - on the test set do head, relation, and tail prediction\n",
    "            )\n",
    "            all_metrics.append(metrics)\n",
    "\n",
    "    # Aggregate metrics across all batches\n",
    "    if model.original_model.task in \"kg_completion\":\n",
    "        if only_relation_prediction:\n",
    "            relation_mean_ranks = []\n",
    "            relation_mrrs = []\n",
    "            relation_hits_at_ks = []\n",
    "            # Collect the metrics from each batch\n",
    "            for metrics in all_metrics:\n",
    "                relation_mean_rank, relation_mrr, relation_hits_at_k = metrics\n",
    "                relation_mean_ranks.append(relation_mean_rank)\n",
    "                relation_mrrs.append(relation_mrr)\n",
    "                relation_hits_at_ks.append(relation_hits_at_k)\n",
    "            # Aggregate the metrics across all batches\n",
    "            relation_mean_rank = sum(relation_mean_ranks) / len(all_metrics)\n",
    "            relation_mrr = sum(relation_mrrs) / len(all_metrics)\n",
    "\n",
    "            # Aggregate the hits@k metrics\n",
    "            all_k_values = set().union(\n",
    "                *(d.keys() for d in relation_hits_at_ks)\n",
    "            )\n",
    "            relation_hits_at_k = {\n",
    "                f\"relation_hits_at_{k}\": sum(\n",
    "                    d.get(k, 0) for d in relation_hits_at_ks\n",
    "                )\n",
    "                / len(all_metrics)\n",
    "                for k in all_k_values\n",
    "            }\n",
    "            # Create the performance_metrics dictionary\n",
    "            performance_metrics = {\n",
    "                \"loss\": total_loss / total_examples,\n",
    "                \"relation_mean_rank\": relation_mean_rank,\n",
    "                \"relation_mrr\": relation_mrr,\n",
    "                **relation_hits_at_k,\n",
    "            }\n",
    "        else:\n",
    "            head_mean_ranks = []\n",
    "            relation_mean_ranks = []\n",
    "            tail_mean_ranks = []\n",
    "            head_mrrs = []\n",
    "            relation_mrrs = []\n",
    "            tail_mrrs = []\n",
    "            head_hits_at_ks = []\n",
    "            relation_hits_at_ks = []\n",
    "            tail_hits_at_ks = []\n",
    "\n",
    "            # Collect the metrics from each batch\n",
    "            for metrics in all_metrics:\n",
    "                (\n",
    "                    head_mean_rank,\n",
    "                    relation_mean_rank,\n",
    "                    tail_mean_rank,\n",
    "                    head_mrr,\n",
    "                    relation_mrr,\n",
    "                    tail_mrr,\n",
    "                    head_hits_at_k,\n",
    "                    relation_hits_at_k,\n",
    "                    tail_hits_at_k,\n",
    "                ) = metrics\n",
    "                head_mean_ranks.append(head_mean_rank)\n",
    "                relation_mean_ranks.append(relation_mean_rank)\n",
    "                tail_mean_ranks.append(tail_mean_rank)\n",
    "                head_mrrs.append(head_mrr)\n",
    "                relation_mrrs.append(relation_mrr)\n",
    "                tail_mrrs.append(tail_mrr)\n",
    "                head_hits_at_ks.append(head_hits_at_k)\n",
    "                relation_hits_at_ks.append(relation_hits_at_k)\n",
    "                tail_hits_at_ks.append(tail_hits_at_k)\n",
    "\n",
    "            # Aggregate the metrics across all batches\n",
    "            head_mean_rank = sum(head_mean_ranks) / len(all_metrics)\n",
    "            relation_mean_rank = sum(relation_mean_ranks) / len(all_metrics)\n",
    "            tail_mean_rank = sum(tail_mean_ranks) / len(all_metrics)\n",
    "            head_mrr = sum(head_mrrs) / len(all_metrics)\n",
    "            relation_mrr = sum(relation_mrrs) / len(all_metrics)\n",
    "            tail_mrr = sum(tail_mrrs) / len(all_metrics)\n",
    "\n",
    "            # Aggregate the hits@k metrics\n",
    "            all_k_values = set().union(\n",
    "                *(d.keys() for d in relation_hits_at_ks)\n",
    "            )\n",
    "            head_hits_at_k = {\n",
    "                f\"head_hits_at_{k}\": sum(d.get(k, 0) for d in head_hits_at_ks)\n",
    "                / len(all_metrics)\n",
    "                for k in all_k_values\n",
    "            }\n",
    "            relation_hits_at_k = {\n",
    "                f\"relation_hits_at_{k}\": sum(\n",
    "                    d.get(k, 0) for d in relation_hits_at_ks\n",
    "                )\n",
    "                / len(all_metrics)\n",
    "                for k in all_k_values\n",
    "            }\n",
    "            tail_hits_at_k = {\n",
    "                f\"tail_hits_at_{k}\": sum(d.get(k, 0) for d in tail_hits_at_ks)\n",
    "                / len(all_metrics)\n",
    "                for k in all_k_values\n",
    "            }\n",
    "\n",
    "            # Create the performance_metrics dictionary\n",
    "            performance_metrics = {\n",
    "                \"loss\": total_loss / total_examples,\n",
    "                \"head_mean_rank\": head_mean_rank,\n",
    "                \"relation_mean_rank\": relation_mean_rank,\n",
    "                \"tail_mean_rank\": tail_mean_rank,\n",
    "                \"head_mrr\": head_mrr,\n",
    "                \"relation_mrr\": relation_mrr,\n",
    "                \"tail_mrr\": tail_mrr,\n",
    "                **head_hits_at_k,\n",
    "                **relation_hits_at_k,\n",
    "                **tail_hits_at_k,\n",
    "            }\n",
    "\n",
    "    elif model.original_model.task == \"node_classification\":\n",
    "        accuracy = sum(all_metrics) / len(all_metrics)\n",
    "        performance_metrics = {\n",
    "            \"loss\": total_loss / total_examples,\n",
    "            \"accuracy\": accuracy,\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"model task isn't valid: {model.original_model.task}\"\n",
    "        )\n",
    "\n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def main(config=None):\n",
    "    run_timestamp = datetime.now().strftime(\"%Y.%m.%d.%H.%M.%S\")\n",
    "    with wandb.init(\n",
    "        project=f\"ScoreMatchingDiffKG_ScoreMatching\",\n",
    "        name=f\"{run_timestamp}_score_matching_model_run\",  # Use a temporary name\n",
    "        config=config if config is not None else {},\n",
    "    ):\n",
    "        config = wandb.config\n",
    "        (\n",
    "            config[\"dataset_name\"],\n",
    "            config[\"embedding_model_name\"],\n",
    "            config[\"task\"],\n",
    "            config[\"aux_dict\"],\n",
    "        ) = extract_info_from_embedding_model_dir(config[\"embedding_model_dir\"])\n",
    "        config[\"prefix\"] = generate_prefix(config, run_timestamp)\n",
    "\n",
    "        wandb.run.name = f\"{config['prefix']}_score_matching_model_run\"\n",
    "        if (\n",
    "            config[\"task\"] == \"node_classification\"\n",
    "            and config[\"dataset_name\"]\n",
    "            not in [\n",
    "                \"Cora\",\n",
    "                \"Citeseer\",\n",
    "                \"Pubmed\",\n",
    "            ]\n",
    "        ) or (\n",
    "            config[\"task\"] != \"node_classification\"\n",
    "            and config[\"dataset_name\"]\n",
    "            in [\n",
    "                \"Cora\",\n",
    "                \"Citeseer\",\n",
    "                \"Pubmed\",\n",
    "            ]\n",
    "        ):\n",
    "            print(f\"Skipping {config['task']} on {config['dataset_name']}\")\n",
    "            return\n",
    "\n",
    "        wandb.run.name = f\"{config['prefix']}_score_matching_model_run\"\n",
    "        score_model = build_model(config)\n",
    "        wandb.watch(score_model)\n",
    "\n",
    "        num_epochs_without_improvement_until_early_finish = config[\n",
    "            \"num_epochs_without_improvement_until_early_finish\"\n",
    "        ]\n",
    "        validate_after_this_many_epochs = config[\n",
    "            \"validate_after_this_many_epochs\"\n",
    "        ]\n",
    "\n",
    "        best_metric_to_optimize = float(\n",
    "            \"inf\"\n",
    "        )  # either val mean rank or val accuracy later\n",
    "        metric_to_optimize = (\n",
    "            \"relation_mean_rank\"\n",
    "            if config[\"task\"] == \"kg_completion\"\n",
    "            else \"accuracy\"\n",
    "        )  # if config[\"task\"] == \"node_classification\"\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "        for epoch in range(config.max_epochs):\n",
    "            loss = train_model(score_model)\n",
    "\n",
    "            if config[\"verbose\"]:\n",
    "                print(f\"Epoch: {epoch:03d}, Train Loss: {loss:.10f}\")\n",
    "\n",
    "\n",
    "            train_metrics = {\"train_epoch\": epoch, \"train_loss\": loss}\n",
    "            if epoch % validate_after_this_many_epochs == 0 and epoch > 0:\n",
    "                val_metrics = test_model(model, val=True)\n",
    "                if val_metrics[metric_to_optimize] <= best_metric_to_optimize: #minimize\n",
    "                    best_metric_to_optimize = val_metrics[metric_to_optimize]\n",
    "                    save_trained_embedding_weights_and_performance(\n",
    "                        model, epoch + 1, val_metrics\n",
    "                    )\n",
    "                    epochs_without_improvement = 0\n",
    "                else:\n",
    "                    epochs_without_improvement += 1\n",
    "\n",
    "                if config[\"verbose\"]:\n",
    "                    metrics_info = \", \".join(\n",
    "                        [\n",
    "                            f'Val {key.replace(\"_\", \" \").title()}: {value:.10f}'\n",
    "                            for key, value in val_metrics.items()\n",
    "                            if isinstance(value, (int, float))\n",
    "                        ]\n",
    "                    )\n",
    "                    print(metrics_info)\n",
    "\n",
    "                if (\n",
    "                    epochs_without_improvement\n",
    "                    >= num_epochs_without_improvement_until_early_finish\n",
    "                ):\n",
    "                    print(\"Stopping early due to increasing training loss.\")\n",
    "                    break\n",
    "\n",
    "                val_metrics = {\n",
    "                    f\"val_{key}\": value for key, value in val_metrics.items()\n",
    "                }\n",
    "\n",
    "            # log to wandb\n",
    "            wandb.log(\n",
    "                {**train_metrics, **val_metrics}\n",
    "                if \"val_metrics\" in locals()\n",
    "                else {**train_metrics}\n",
    "            )\n",
    "\n",
    "        model.load_state_dict(\n",
    "            torch.load(\n",
    "                osp.join(\n",
    "                    model.save_path,\n",
    "                    f\"{model.config['prefix']}_score_matching_model_weights.pth\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        test_metrics = test_model(model)\n",
    "\n",
    "        if config[\"verbose\"]:\n",
    "            test_metrics_info = \", \".join(\n",
    "                [\n",
    "                    f'Test {key.replace(\"_\", \" \").title()}: {value:.4f}'\n",
    "                    for key, value in test_metrics.items()\n",
    "                    if isinstance(value, (int, float))\n",
    "                ]\n",
    "            )\n",
    "            print(test_metrics_info)\n",
    "\n",
    "        with open(\n",
    "            osp.join(\n",
    "                model.save_path,\n",
    "                f\"{model.config['prefix']}_score_matching_model_performance.txt\",\n",
    "            ),\n",
    "            \"a\",\n",
    "        ) as file:\n",
    "            file.write(\"Test Metrics:\\n\")\n",
    "            for metric, value in test_metrics.items():\n",
    "                file.write(f\"{metric}: {value}\\n\")\n",
    "\n",
    "        test_metrics = {\n",
    "            f\"test_{key}\": value for key, value in test_metrics.items()\n",
    "        }\n",
    "        wandb.log({**test_metrics})\n",
    "        wandb.save(model.save_path, base_path=parent_dir)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sweep_or_main(\n",
    "    run_sweep=run_sweep,\n",
    "    project_name=\"ScoreMatchingDiffKG_Score_Sweep\",\n",
    "    main=main,\n",
    "    config=config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ScoreMatchingDiffKG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
