{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Blaine Hill\n",
    "\n",
    "In this notebook, we train a score matching model as described by [Yang Song](https://yang-song.net/blog/2021/score/) to do KG Reasoning tasks such as [link prediction](https://paperswithcode.com/task/link-prediction)\n",
    "\n",
    "$$s_\\theta(x) = \\nabla_x \\log p_\\theta(x) = -\\nabla_x f_\\theta(x) - \\nabla_x \\log Z_\\theta = -\\nabla_x f_\\theta(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"score_matching_model.ipynb\"\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "# wandb.login(relogin=True)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import sys\n",
    "import os.path as osp\n",
    "from ipykernel import get_connection_file\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "notebook_path = osp.abspath(\n",
    "    osp.join(os.getcwd(), osp.basename(get_connection_file()))\n",
    ")\n",
    "parent_dir = osp.dirname(osp.dirname(notebook_path))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Import utility functions and model utilities\n",
    "from utils.utils import *\n",
    "from utils.score_matching_model_utils import *\n",
    "# from utils.score_matching_model.utils import *\n",
    "\n",
    "from utils.score_matching_models.ScoreModel import ScoreModel\n",
    "\n",
    "\n",
    "set_seed()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sweep = True\n",
    "if run_sweep:\n",
    "    with open(\"sweep_config.yaml\", \"r\") as file:\n",
    "        sweep_config = yaml.safe_load(file)\n",
    "else:\n",
    "    config = {\n",
    "        \"embedding_model_dir\": \"../trained_embedding_models/YAGO3_10_ComplEx_relation_prediction_2024.05.13.21.03.48_1cce66c3_embedding_model/\",\n",
    "        \"score_model_hidden_dim\": 512,\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\": 3,\n",
    "        \"num_timesteps\": 20,  # Adjust based on your desired SDE iterations\n",
    "        \"lr\": 1e-4,\n",
    "        \"similarity_metric\": \"cosine\",\n",
    "        \"verbose\": True,\n",
    "        \"k\": [1, 3, 10],  # used for top-k evaluation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config, view_english=False):\n",
    "    embedding_model_dir = config[\"embedding_model_dir\"]\n",
    "    embedding_model, embedding_model_name, dataset_name = (\n",
    "        initialize_trained_embedding_model(embedding_model_dir)\n",
    "    )\n",
    "    embedding_model.eval()\n",
    "\n",
    "    train_data, val_data, test_data, data_path = load_dataset(\n",
    "        dataset_name, parent_dir=parent_dir, device=device\n",
    "    )\n",
    "\n",
    "    original_model = ScoreModel(\n",
    "        embedding_model,\n",
    "        config[\"score_model_hidden_dim\"],\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    # Calculate batch size based on the number of GPUs available\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    batch_size_per_gpu = config[\"batch_size\"]\n",
    "    batch_size = (\n",
    "        batch_size_per_gpu * num_gpus if num_gpus > 0 else batch_size_per_gpu\n",
    "    )\n",
    "    num_workers = num_gpus if num_gpus > 0 else 1\n",
    "\n",
    "    # Helper function to prepare loader arguments\n",
    "    def prepare_loader_args(data, batch_size, num_workers):\n",
    "        \"\"\"Prepares and returns loader arguments based on the data and configuration.\"\"\"\n",
    "        loader_args = {\n",
    "            \"head_index\": data.edge_index[0],\n",
    "            \"rel_type\": data.edge_type,\n",
    "            \"tail_index\": data.edge_index[1],\n",
    "            \"batch_size\": batch_size,\n",
    "            \"shuffle\": True,\n",
    "            \"num_workers\": num_workers,\n",
    "        }\n",
    "        # Add extra features if available\n",
    "        if hasattr(data, \"x\"):\n",
    "            loader_args[\"x\"] = data.x\n",
    "        if hasattr(data, \"y\"):\n",
    "            loader_args[\"y\"] = data.y\n",
    "        return loader_args\n",
    "\n",
    "    # Prepare loader arguments for train, validation, and test datasets\n",
    "    train_loader_args = prepare_loader_args(\n",
    "        train_data, batch_size, num_workers\n",
    "    )\n",
    "    val_loader_args = prepare_loader_args(val_data, batch_size, num_workers)\n",
    "    test_loader_args = prepare_loader_args(test_data, batch_size, num_workers)\n",
    "\n",
    "    # Create data loaders for training, validation, and testing\n",
    "    train_loader = DataLoader(**train_loader_args)\n",
    "    val_loader = DataLoader(**val_loader_args)\n",
    "    test_loader = DataLoader(**test_loader_args)\n",
    "\n",
    "    # # Create data loaders\n",
    "    # train_dataset = TensorDataset(\n",
    "    #     train_data.edge_index[0],\n",
    "    #     train_data.edge_type,\n",
    "    #     train_data.edge_index[1],\n",
    "    # )\n",
    "    # train_loader = DataLoader(\n",
    "    #     train_dataset, batch_size=config[\"batch_size\"], shuffle=True\n",
    "    # )\n",
    "\n",
    "    # val_dataset = TensorDataset(\n",
    "    #     val_data.edge_index[0], val_data.edge_type, val_data.edge_index[1]\n",
    "    # )\n",
    "    # val_loader = DataLoader(\n",
    "    #     val_dataset, batch_size=config[\"batch_size\"], shuffle=False\n",
    "    # )\n",
    "\n",
    "    # test_dataset = TensorDataset(\n",
    "    #     test_data.edge_index[0], test_data.edge_type, test_data.edge_index[1]\n",
    "    # )\n",
    "    # test_loader = DataLoader(\n",
    "    #     test_dataset, batch_size=config[\"batch_size\"], shuffle=False\n",
    "    # )\n",
    "\n",
    "    optimizer = Adam(score_model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # Use DataParallel for multi-GPU setups on single server\n",
    "    if num_gpus > 1:\n",
    "        score_model = torch.nn.DataParallel(original_model)\n",
    "    else:\n",
    "        score_model = original_model\n",
    "\n",
    "    if view_english:\n",
    "        entity_dict, relation_dict = load_dicts(\n",
    "            data_path\n",
    "        )  # note: if you want to load english for RotatE, store the dicts to map from indices to freebase ids/ relations in data_path/processed\n",
    "        score_model.entity_dict = entity_dict\n",
    "        score_model.relation_dict = relation_dict\n",
    "\n",
    "    score_model.train_data = train_data\n",
    "    score_model.val_data = val_data\n",
    "    score_model.test_data = test_data\n",
    "    score_model.train_loader = train_loader\n",
    "    score_model.val_loader = val_loader\n",
    "    score_model.test_loader = test_loader\n",
    "    score_model.optimizer = optimizer\n",
    "    score_model.config = config\n",
    "    score_model.dataset_name = dataset_name\n",
    "    score_model.embedding_model_name = embedding_model_name\n",
    "    score_model.embedding_model_dir = embedding_model_dir\n",
    "    score_model.original_model = (\n",
    "        original_model  # Keep a reference to the original model\n",
    "    )\n",
    "\n",
    "    # Determine the path to save results and create directory if it does not exist\n",
    "    if \"save_path\" in config:\n",
    "        save_path = config[\"save_path\"]\n",
    "        if not os.path.exists(save_path):\n",
    "            raise ValueError(\n",
    "                f\"Directory {save_path} does not exist in which to save the trained score matching models. Please create it before saving.\"\n",
    "            )\n",
    "        save_path = osp.join(\n",
    "            save_path,\n",
    "            f\"{config['prefix']}_score_matching_model\",\n",
    "        )\n",
    "    else:\n",
    "        save_path = osp.join(\n",
    "            parent_dir,\n",
    "            \"trained_score_matching_models\",\n",
    "            f\"{config['prefix']}_score_matching_model\",\n",
    "        )\n",
    "    \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    model.save_path = save_path\n",
    "\n",
    "    # Save the model configuration\n",
    "    save_embedding_model_config(model)\n",
    "\n",
    "\n",
    "# def denoising_score_matching_loss(score_model, h, r, t, timestep):\n",
    "#     \"\"\"\n",
    "#     Denoising score-matching loss with noise-conditional score networks.\n",
    "#     \"\"\"\n",
    "#     h_emb, r_emb, t_emb = (\n",
    "#         score_model.embedding_model.node_emb(h),\n",
    "#         score_model.embedding_model.rel_emb(r),\n",
    "#         score_model.embedding_model.node_emb(t),\n",
    "#     )\n",
    "#     true_score = score_model(\n",
    "#         h_emb, r_emb, t_emb\n",
    "#     )  # simply do not pass in the timestep\n",
    "#     noisy_score = score_model(h_emb, r_emb, t_emb, timestep)\n",
    "#     return ((true_score - noisy_score) ** 2).mean()\n",
    "\n",
    "\n",
    "def train_model(score_model):\n",
    "    score_model.train()\n",
    "    loss_epoch = 0\n",
    "    for batch in score_model.train_loader:\n",
    "            # Extract data from the batch\n",
    "            h, r, t = (\n",
    "                batch[\"head_index\"],\n",
    "                batch[\"rel_type\"],\n",
    "                batch[\"tail_index\"],\n",
    "            )\n",
    "            x = batch.get(\"x\", None)\n",
    "            y = batch.get(\"y\", None)\n",
    "\n",
    "            # Move data to the appropriate device\n",
    "            h, r, t = (\n",
    "                h.to(device),\n",
    "                r.to(device),\n",
    "                t.to(device),\n",
    "            )\n",
    "\n",
    "        for timestep in range(score_model.config[\"num_timesteps\"]):\n",
    "            loss = score_model.original_model.loss(\n",
    "                score_model, h, r, t, timestep, x, y, task\n",
    "            )\n",
    "            loss.backward()\n",
    "        score_model.optimizer.step()\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "    return loss_epoch / len(score_model.train_loader)\n",
    "\n",
    "\n",
    "def test_model(score_model, val=False):\n",
    "    score_model.eval()\n",
    "    test_loss = 0\n",
    "    loader = score_model.val_loader if val else score_model.test_loader\n",
    "    all_metrics = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # Extract data from the batch\n",
    "            h, r, t = (\n",
    "                batch[\"head_index\"],\n",
    "                batch[\"rel_type\"],\n",
    "                batch[\"tail_index\"],\n",
    "            )\n",
    "            x = batch.get(\"x\", None)\n",
    "            y = batch.get(\"y\", None)\n",
    "\n",
    "            # Move data to the appropriate device\n",
    "            h, r, t = (\n",
    "                h.to(device),\n",
    "                r.to(device),\n",
    "                t.to(device),\n",
    "            )\n",
    "            if x is not None:\n",
    "                x = x.to(device)\n",
    "            if y is not None:\n",
    "                y = y.to(device)\n",
    "            for timestep in range(score_model.config[\"num_timesteps\"]):\n",
    "                loss = score_model.original_model.loss(\n",
    "                    score_model, h, r, t, timestep\n",
    "                )\n",
    "                test_loss += loss.item()\n",
    "\n",
    "            # Calculate metrics for the batch\n",
    "            metrics = model.original_model.test(\n",
    "                head_index=h,\n",
    "                rel_type=r,\n",
    "                tail_index=t,\n",
    "                x=x,\n",
    "                y=y,\n",
    "                batch_size=model.config[\"batch_size\"],\n",
    "                k=model.config[\"k\"],\n",
    "                task=model.original_model.config[\"task\"],\n",
    "            )\n",
    "            all_metrics.append(metrics)\n",
    "\n",
    "    # Aggregate metrics across all batches\n",
    "    if model.original_model.config[\"task\"] in [\"relation_prediction\", \"head_prediction\", \"tail_prediction\"]:\n",
    "        mean_rank = sum(m[0] for m in all_metrics) / len(all_metrics)\n",
    "        mrr = sum(m[1] for m in all_metrics) / len(all_metrics)\n",
    "        hits_at_k = {k: sum(m[2][k] for m in all_metrics) / len(all_metrics) for k in m[2]}\n",
    "        performance_metrics = {\n",
    "            \"loss\": total_loss / total_examples,\n",
    "            \"mean_rank\": mean_rank,\n",
    "            \"mrr\": mrr,\n",
    "        }\n",
    "        for k_value, hits_value in hits_at_k.items():\n",
    "            performance_metrics[f\"hits_at_{k_value}\"] = hits_value\n",
    "\n",
    "    elif model.original_model.config[\"task\"] == \"node_classification\":\n",
    "        accuracy = sum(all_metrics) / len(all_metrics)\n",
    "        performance_metrics = {\n",
    "            \"loss\": total_loss / total_examples,\n",
    "            \"accuracy\": accuracy,\n",
    "        }\n",
    "    \n",
    "    else: \n",
    "        raise ValueError(f\"model task isn't valid: {model.original_model.config[\"task\"]}\")\n",
    "\n",
    "    return performance_metrics\n",
    "\n",
    "\n",
    "def reverse_sde_link_prediction(h_emb, t_emb, score_model):\n",
    "    \"\"\"\n",
    "    Refine the relation embedding using reverse SDE for better link prediction.\n",
    "    h_emb: Embeddings of the head entities (batch_size, node_emb_dim).\n",
    "    t_emb: Embeddings of the tail entities (batch_size, node_emb_dim).\n",
    "    score_model: The trained score model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize with random noise\n",
    "    r_emb = torch.randn(h_emb.size(0), score_model.rel_emb_dim, device=device)\n",
    "\n",
    "    # Define the time steps for the reverse SDE process\n",
    "    t_steps = torch.linspace(\n",
    "        score_model.config[\"num_timesteps\"] - 1, 0, score_model.config[\"num_timesteps\"]\n",
    "    ).to(device)\n",
    "\n",
    "    # Define the SDE function for the reverse process\n",
    "    def sde_func(t, r_emb):\n",
    "        with torch.enable_grad():\n",
    "            r_emb.requires_grad_(True)\n",
    "            score = score_model(h_emb, r_emb, t_emb, t)\n",
    "            grad_r_emb = torch.autograd.grad(\n",
    "                score.sum(dim=0), r_emb, create_graph=True\n",
    "            )[0]\n",
    "        return -grad_r_emb  # Reverse direction\n",
    "\n",
    "    # Perform the reverse SDE integration\n",
    "    with torch.no_grad():\n",
    "        for t in reversed(t_steps):\n",
    "            r_emb = sde_func(t, r_emb)\n",
    "\n",
    "    return r_emb  # returns predictions for r of shape batch, rel_emb_dim\n",
    "\n",
    "\n",
    "def compute_link_prediction_metrics(\n",
    "    score_model, val=False, view_english=False\n",
    "):\n",
    "    all_ranks = []\n",
    "    all_hits_at_1, all_hits_at_3, all_hits_at_10 = [], [], []\n",
    "\n",
    "    # Get all relation embeddings from the model\n",
    "    all_relations = score_model.embedding_model.rel_emb.weight.detach()\n",
    "    loader = score_model.val_loader if val else score_model.test_loader\n",
    "    for batch in loader:\n",
    "        h, true_r, t = batch\n",
    "        h, true_r, t = h.to(device), true_r.to(device), t.to(device)\n",
    "\n",
    "        h_emb = score_model.embedding_model.node_emb(h)\n",
    "        t_emb = score_model.embedding_model.node_emb(t)\n",
    "        # Assuming reverse_sde_link_prediction function returns refined relation embeddings\n",
    "        refined_r_emb = reverse_sde_link_prediction(h_emb, t_emb, score_model)\n",
    "\n",
    "        if score_model.config[\"similarity_metric\"] == \"cosine\":\n",
    "            # Calculate cosine similarity\n",
    "            similarity = F.cosine_similarity(\n",
    "                refined_r_emb.unsqueeze(1), all_relations.unsqueeze(0), dim=-1\n",
    "            )\n",
    "\n",
    "            # Since cosine similarity is higher for closer vectors, we use largest=True\n",
    "            values1, indices1 = similarity.topk(1, dim=1, largest=True)\n",
    "            values3, indices3 = similarity.topk(3, dim=1, largest=True)\n",
    "            values10, indices10 = similarity.topk(10, dim=1, largest=True)\n",
    "        elif score_model.config[\"similarity_metric\"] == \"l2\":\n",
    "            # Calculate Euclidean distance\n",
    "            dist = torch.norm(\n",
    "                refined_r_emb.unsqueeze(1) - all_relations.unsqueeze(0),\n",
    "                p=2,\n",
    "                dim=-1,\n",
    "            )\n",
    "\n",
    "            # For Euclidean distance, closer vectors have smaller distances, so largest=False\n",
    "            values1, indices1 = dist.topk(1, dim=1, largest=False)\n",
    "            values3, indices3 = dist.topk(3, dim=1, largest=False)\n",
    "            values10, indices10 = dist.topk(10, dim=1, largest=False)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Haven't implemented a similarity metric yet for {score_model.config['similarity_metric']}\"\n",
    "            )\n",
    "        if view_english and score_model.dataset_name == \"FB15k_237\":\n",
    "            # for getting english from freebase ID for FB15k_237 data\n",
    "            # show_only_first=True means that only the first entry in the batch will be processed and returned in english\n",
    "            ic(\n",
    "                convert_indices_to_english(\n",
    "                    indices10, score_model.relation_dict, is_entities=False\n",
    "                )\n",
    "            )\n",
    "            ic(\n",
    "                convert_indices_to_english(\n",
    "                    true_r, score_model.relation_dict, is_entities=False\n",
    "                )\n",
    "            )\n",
    "            ic(convert_indices_to_english(h, score_model.entity_dict))\n",
    "            ic(convert_indices_to_english(t, score_model.entity_dict))\n",
    "\n",
    "        # Check if true relation is within the top K predictions\n",
    "        hits_at_1 = (\n",
    "            (indices1 == true_r.unsqueeze(1)).any(dim=1).float().mean().item()\n",
    "        )\n",
    "        all_hits_at_1.append(hits_at_1)\n",
    "        hits_at_3 = (\n",
    "            (indices3 == true_r.unsqueeze(1)).any(dim=1).float().mean().item()\n",
    "        )\n",
    "        all_hits_at_3.append(hits_at_3)\n",
    "        hits_at_10 = (\n",
    "            (indices10 == true_r.unsqueeze(1)).any(dim=1).float().mean().item()\n",
    "        )\n",
    "        all_hits_at_10.append(hits_at_10)\n",
    "\n",
    "        # Calculate rank of the true relation\n",
    "        for i in range(indices1.size(0)):\n",
    "            true_relation_idx = (indices10[i] == true_r[i]).nonzero(\n",
    "                as_tuple=True\n",
    "            )[0]\n",
    "            if true_relation_idx.numel() > 0:\n",
    "                all_ranks.append(true_relation_idx.item() + 1)\n",
    "\n",
    "    mean_rank = np.mean(all_ranks)\n",
    "    mrr = np.mean([1.0 / rank for rank in all_ranks if rank > 0])\n",
    "    hits_at_1 = np.mean(all_hits_at_1)\n",
    "    hits_at_3 = np.mean(all_hits_at_3)\n",
    "    hits_at_10 = np.mean(all_hits_at_10)\n",
    "\n",
    "    link_prediction_performance_metrics = {\n",
    "        \"mean_rank\": mean_rank,\n",
    "        \"mrr\": mrr,\n",
    "        \"hits_at_1\": hits_at_1,\n",
    "        \"hits_at_3\": hits_at_3,\n",
    "        \"hits_at_10\": hits_at_10,\n",
    "    }\n",
    "\n",
    "    return link_prediction_performance_metrics\n",
    "\n",
    "    \n",
    "def main(config=None):\n",
    "    run_timestamp = datetime.now().strftime(\"%Y.%m.%d.%H.%M.%S\")\n",
    "    with wandb.init(\n",
    "        project=f\"ScoreMatchingDiffKG_ScoreMatching\",\n",
    "        name=f\"{run_timestamp}_score_matching_model_run\",  # Use a temporary name\n",
    "        config=config if config is not None else {},\n",
    "    ):\n",
    "        config = wandb.config\n",
    "        config[\"dataset_name\"], config[\"embedding_model_name\"], config[\"task\"] = extract_info_from_string(config[\"embedding_model_dir\"])\n",
    "        config[\"prefix\"] = (\n",
    "            f'{config[\"dataset_name\"]}_{config[\"embedding_model_name\"]}_{config[\"task\"]}_{run_timestamp}_{generate_unique_string(config)}'\n",
    "        )\n",
    "        config[\"prefix\"] = generate_prefix(config, run_timestamp)\n",
    "\n",
    "\n",
    "        wandb.run.name = f\"{config['prefix']}_score_matching_model_run\"\n",
    "        score_model = build_model(config)\n",
    "        wandb.watch(score_model)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            loss = train_model(score_model)\n",
    "            if config[\"verbose\"]:\n",
    "                print(f\"Epoch: {epoch:03d}, Train Loss: {loss:.4f}\")\n",
    "            train_metrics = {\"train_epoch\": epoch, \"train_loss\": loss}\n",
    "            if epoch % 10 == 0 and epoch > 0:\n",
    "                # set val to be True since we are running over our validation data\n",
    "                val = True\n",
    "                val_loss = test_model(score_model, val=val)\n",
    "                link_prediction_performance_metrics = (\n",
    "                    compute_link_prediction_metrics(score_model, val=val)\n",
    "                )\n",
    "                if config[\"verbose\"]:\n",
    "                    print(\n",
    "                        f'Val Mean Rank: {link_prediction_performance_metrics[\"mean_rank\"]:.2f}, Val Mean Reciprocal Rank: {link_prediction_performance_metrics[\"mrr\"]:.2f}, Val Hits@1: {link_prediction_performance_metrics[\"hits_at_1\"]:.4f}, Val Hits@3: {link_prediction_performance_metrics[\"hits_at_3\"]:.4f}, Val Hits@10: {link_prediction_performance_metrics[\"hits_at_10\"]:.4f}'\n",
    "                    )\n",
    "                val_metrics = {\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"val_mean_rank\": link_prediction_performance_metrics[\n",
    "                        \"mean_rank\"\n",
    "                    ],\n",
    "                    \"val_mrr\": link_prediction_performance_metrics[\"mrr\"],\n",
    "                    \"val_hits_at_1\": link_prediction_performance_metrics[\n",
    "                        \"hits_at_1\"\n",
    "                    ],\n",
    "                    \"val_hits_at_3\": link_prediction_performance_metrics[\n",
    "                        \"hits_at_3\"\n",
    "                    ],\n",
    "                    \"val_hits_at_10\": link_prediction_performance_metrics[\n",
    "                        \"hits_at_10\"\n",
    "                    ],\n",
    "                }\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model_state = score_model.state_dict()\n",
    "\n",
    "            # log to wandb\n",
    "            wandb.log(\n",
    "                {**train_metrics, **val_metrics}\n",
    "                if \"val_metrics\" in locals()\n",
    "                else {**train_metrics}\n",
    "            )\n",
    "\n",
    "        if best_model_state:\n",
    "            score_model.load_state_dict(best_model_state)\n",
    "\n",
    "            # once everything is finished, test model\n",
    "            test_loss = test_model(score_model)\n",
    "            link_prediction_performance_metrics = (\n",
    "                compute_link_prediction_metrics(score_model)\n",
    "            )\n",
    "            if config[\"verbose\"]:\n",
    "                print(\n",
    "                    f'Test Mean Rank: {link_prediction_performance_metrics[\"mean_rank\"]:.2f}, Test Mean Reciprocal Rank: {link_prediction_performance_metrics[\"mrr\"]:.2f}, Test Hits@1: {link_prediction_performance_metrics[\"hits_at_1\"]:.4f}, Test Hits@3: {link_prediction_performance_metrics[\"hits_at_3\"]:.4f}, Test Hits@10: {link_prediction_performance_metrics[\"hits_at_10\"]:.4f}'\n",
    "                )\n",
    "            test_metrics = {\n",
    "                \"test_loss\": test_loss,\n",
    "                \"test_mean_rank\": link_prediction_performance_metrics[\n",
    "                    \"mean_rank\"\n",
    "                ],\n",
    "                \"test_mrr\": link_prediction_performance_metrics[\"mrr\"],\n",
    "                \"test_hits_at_1\": link_prediction_performance_metrics[\n",
    "                    \"hits_at_1\"\n",
    "                ],\n",
    "                \"test_hits_at_3\": link_prediction_performance_metrics[\n",
    "                    \"hits_at_3\"\n",
    "                ],\n",
    "                \"test_hits_at_10\": link_prediction_performance_metrics[\n",
    "                    \"hits_at_10\"\n",
    "                ],\n",
    "            }\n",
    "            wandb.log({**test_metrics})\n",
    "\n",
    "            # Save the trained model\n",
    "            path = osp.join(\n",
    "                os.getcwd(),\n",
    "                f\"{score_model.dataset_name}_score_matching_model_weights.pth\",\n",
    "            )\n",
    "            torch.save(score_model.state_dict(), path)\n",
    "\n",
    "            score_model.load_state_dict(best_model_state)\n",
    "\n",
    "            # Fetch a batch from the train_loader and prepare it\n",
    "            batch = fetch_and_prepare_batch(score_model.train_loader, device)\n",
    "            head_index, rel_type, tail_index = batch  # Unpack the batch\n",
    "\n",
    "            # Define the file path for the ONNX model\n",
    "            file_path = f\"{config['dataset_name']}_best_score_matching_model_weights.onnx\"\n",
    "\n",
    "            # Export the model to ONNX format using the fetched batch as dummy inputs\n",
    "            export_model_to_onnx(\n",
    "                model,\n",
    "                (head_index, rel_type, tail_index),\n",
    "                file_path,\n",
    "                input_names=[\"head_index\", \"rel_type\", \"tail_index\"],\n",
    "                dynamic_axes={\n",
    "                    \"head_index\": {0: \"batch_size\"},\n",
    "                    \"rel_type\": {0: \"batch_size\"},\n",
    "                    \"tail_index\": {0: \"batch_size\"},\n",
    "                },\n",
    "            )\n",
    "\n",
    "            wandb.save(\n",
    "                f\"{score_model.dataset_name}_score_matching_model_weights.onnx\"\n",
    "            )\n",
    "\n",
    "        return score_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sweep:\n",
    "\n",
    "    with open(\"sweep_config.yaml\", \"r\") as file:\n",
    "        sweep_config = yaml.safe_load(file)\n",
    "\n",
    "    sweep_id = wandb.sweep(\n",
    "        project=f\"ScoreMatchingDiffKG_Score_Sweep\", sweep=sweep_config\n",
    "    )\n",
    "\n",
    "    wandb.agent(sweep_id, function=main)\n",
    "else:\n",
    "    model = main(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ScoreMatchingDiffKG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
