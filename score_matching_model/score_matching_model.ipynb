{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Blaine Hill\n",
    "\n",
    "In this notebook, we train a score matching model as described by [Yang Song](https://yang-song.net/blog/2021/score/)\n",
    "\n",
    "\n",
    "$$s_\\theta(x) = \\nabla_x \\log p_\\theta(x) = -\\nabla_x f_\\theta(x) - \\nabla_x \\log Z_\\theta = -\\nabla_x f_\\theta(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add probability flow ode and use sde for noise\n",
    "\n",
    "#TODO add wandb functionality\n",
    "\n",
    "#TODO abstract any dimensions \n",
    "\n",
    "#TODO add conditional generation with annealed langevin sampling (essentially the sde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from torch_geometric.nn import RotatE\n",
    "# from torch_geometric.datasets import FB15k_237\n",
    "# import sys\n",
    "# import os\n",
    "# import os.path as osp\n",
    "# import math\n",
    "# import numpy as np\n",
    "# from icecream import ic\n",
    "\n",
    "# # Set up paths\n",
    "# parent_dir = os.path.abspath(os.pardir)\n",
    "# sys.path.append(parent_dir)\n",
    "# data_path = osp.join(parent_dir, \"data\", \"FB15k_237\")\n",
    "\n",
    "# # Load dataset\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# train_data = FB15k_237(data_path, split=\"train\")[0].to(device)\n",
    "# val_data = FB15k_237(data_path, split=\"val\")[0].to(device)\n",
    "# test_data = FB15k_237(data_path, split=\"test\")[0].to(device)\n",
    "\n",
    "# # Load pre-trained RotatE model\n",
    "# model = RotatE(num_nodes=train_data.num_nodes, num_relations=train_data.num_edge_types, hidden_channels=50).to(device)\n",
    "# model.load_state_dict(torch.load(\"../embedding_model/FB15k_237_RotatE_embedding_model_weights.pth\"))\n",
    "# model.eval()\n",
    "\n",
    "# # Get entity and relation embeddings\n",
    "# with torch.no_grad():\n",
    "#     entity_embeddings_real = model.node_emb.weight.detach()\n",
    "#     entity_embeddings_im = model.node_emb_im.weight.detach()\n",
    "#     entity_embeddings = torch.cat([entity_embeddings_real, entity_embeddings_im], dim=-1)\n",
    "\n",
    "#     relation_embeddings = model.rel_emb.weight.detach()\n",
    "\n",
    "# ic(entity_embeddings_real.shape)\n",
    "# ic(entity_embeddings_im.shape)\n",
    "# ic(entity_embeddings.shape)\n",
    "\n",
    "# ic(relation_embeddings.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class ScoreModel(nn.Module):\n",
    "#     def __init__(self, embedding_dim):\n",
    "#         super(ScoreModel, self).__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(embedding_dim * 2 + 50, 512),  # Assuming concatenation of h, r, and t embeddings\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 1)  # Outputting a single score\n",
    "#         )\n",
    "\n",
    "#     def forward(self, h, r, t):\n",
    "#         x = torch.cat([h, r, t], dim=-1)\n",
    "#         return self.net(x)\n",
    "\n",
    "# def sde(h, r, t, beta):\n",
    "#     # This is a simplified SDE for demonstration. You might need a more complex one.\n",
    "#     drift = score_function(h, r, t)  # Assuming score_function is defined\n",
    "#     diffusion = beta  # This could be a function of time or constant\n",
    "#     return drift, diffusion\n",
    "\n",
    "# def rsde(h, r, t, beta):\n",
    "#     drift, _ = sde(h, r, t, beta)\n",
    "#     reverse_drift = -drift + 2 * diffusion  # Simplified reverse drift\n",
    "#     return reverse_drift, diffusion\n",
    "\n",
    "# def annealed_langevin_sampling(h, r, t, beta_schedule, num_steps=1000):\n",
    "#     samples = torch.randn_like(h)  # Initialize with noise\n",
    "#     for beta in beta_schedule:\n",
    "#         drift, diffusion = rsde(h, r, t, beta)\n",
    "#         samples += drift + torch.sqrt(diffusion) * torch.randn_like(samples)\n",
    "#     return samples\n",
    "\n",
    "# beta_schedule = torch.linspace(start=0.1, end=20, steps=num_steps)\n",
    "\n",
    "# optimizer = optim.Adam(score_model.parameters(), lr=1e-3)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for h, r, t in data_loader:  # Assuming data_loader is defined\n",
    "#         optimizer.zero_grad()\n",
    "#         scores = score_model(h, r, t)\n",
    "#         loss = compute_loss(scores)  # Define compute_loss based on your specific task\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     # Evaluate on link prediction tasks\n",
    "# def compute_metrics(data, entity_embeddings, relation_embeddings):\n",
    "#     mean_rank = []\n",
    "#     mean_reciprocal_rank = []\n",
    "#     hits_at_k = {k: [] for k in [1, 3, 10]}\n",
    "\n",
    "#     for h, r, t in data:\n",
    "#         scores = []\n",
    "#         for candidate_tail in range(train_data.num_nodes):\n",
    "#             score = score_function(entity_embeddings[h], relation_embeddings[r], entity_embeddings[candidate_tail])\n",
    "#             scores.append(score)\n",
    "\n",
    "#         true_score = scores[t]\n",
    "#         ranks = 1 + torch.argsort(torch.tensor(scores), descending=True).tolist().index(t)\n",
    "#         mean_rank.append(ranks)\n",
    "#         mean_reciprocal_rank.append(1 / ranks)\n",
    "\n",
    "#         for k in hits_at_k:\n",
    "#             if ranks <= k:\n",
    "#                 hits_at_k[k].append(1)\n",
    "#             else:   \n",
    "#                 hits_at_k[k].append(0)\n",
    "\n",
    "#     mean_rank = np.mean(mean_rank)\n",
    "#     mean_reciprocal_rank = np.mean(mean_reciprocal_rank)\n",
    "#     hits_at_k = {k: np.mean(v) for k, v in hits_at_k.items()}\n",
    "\n",
    "#     return mean_rank, mean_reciprocal_rank, hits_at_k\n",
    "\n",
    "# # Compute metrics for the original embeddings\n",
    "# original_mean_rank, original_mean_reciprocal_rank, original_hits_at_k = compute_metrics(test_data, entity_embeddings, model.relation_emb.weight)\n",
    "# print(\"Original Embeddings:\")\n",
    "# print(f\"Mean Rank: {original_mean_rank}\")\n",
    "# print(f\"Mean Reciprocal Rank: {original_mean_reciprocal_rank}\")\n",
    "# print(f\"Hits@1: {original_hits_at_k[1]}, Hits@3: {original_hits_at_k[3]}, Hits@10: {original_hits_at_k[10]}\")\n",
    "\n",
    "# # Compute metrics for the conditioned embeddings\n",
    "# conditioned_mean_rank, conditioned_mean_reciprocal_rank, conditioned_hits_at_k = compute_metrics(test_data, entity_embeddings_conditioned, model.relation_emb.weight)\n",
    "# print(\"\\nConditioned Embeddings:\")\n",
    "# print(f\"Mean Rank: {conditioned_mean_rank}\")\n",
    "# print(f\"Mean Reciprocal Rank: {conditioned_mean_reciprocal_rank}\")\n",
    "# print(f\"Hits@1: {conditioned_hits_at_k[1]}, Hits@3: {conditioned_hits_at_k[3]}, Hits@10: {conditioned_hits_at_k[10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim import Adam\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from torch_geometric.nn import RotatE\n",
    "# from torch_geometric.datasets import FB15k_237\n",
    "# import sys\n",
    "# import os\n",
    "# import os.path as osp\n",
    "# import math\n",
    "# import numpy as np\n",
    "# from icecream import ic\n",
    "\n",
    "# # Set up paths\n",
    "# parent_dir = os.path.abspath(os.pardir)\n",
    "# sys.path.append(parent_dir)\n",
    "# data_path = osp.join(parent_dir, \"data\", \"FB15k_237\")\n",
    "\n",
    "# # Load dataset\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# train_data = FB15k_237(data_path, split=\"train\")[0].to(device)\n",
    "# val_data = FB15k_237(data_path, split=\"val\")[0].to(device)\n",
    "# test_data = FB15k_237(data_path, split=\"test\")[0].to(device)\n",
    "\n",
    "\n",
    "# # Define cosine noise schedule\n",
    "# def cosine_beta_schedule(timesteps, s=0.008):\n",
    "#     \"\"\"\n",
    "#     Cosine schedule as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "#     \"\"\"\n",
    "#     steps = timesteps + 1\n",
    "#     x = torch.linspace(0, timesteps, steps)\n",
    "#     alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "#     alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "#     betas = 1 - alphas_cumprod[1:] / alphas_cumprod[:-1]\n",
    "#     return torch.clip(betas, 0, 0.999)\n",
    "    \n",
    "\n",
    "\n",
    "# class ScoreModel(nn.Module):\n",
    "#     def __init__(self, embedding_model, embedding_dim, relation_dim):\n",
    "#         super(ScoreModel, self).__init__()\n",
    "#         self.embedding_model = embedding_model\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.relation_dim = relation_dim\n",
    "#         self.score_net = nn.Sequential(\n",
    "#             nn.Linear(embedding_dim + relation_dim, 512, dtype=torch.float),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 512, dtype=torch.float),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 1, dtype=torch.float)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, h, r, t, noise_level=None):\n",
    "#         h_emb = self.embedding_model.node_emb(h)\n",
    "#         r_emb = self.embedding_model.rel_emb(r)\n",
    "#         t_emb = self.embedding_model.node_emb(t)\n",
    "\n",
    "#         if noise_level is not None:\n",
    "#             noise_h = torch.randn_like(h_emb) * noise_level\n",
    "#             noise_r = torch.randn_like(r_emb) * noise_level\n",
    "#             noise_t = torch.randn_like(t_emb) * noise_level\n",
    "#             h_emb = h_emb + noise_h\n",
    "#             r_emb = r_emb + noise_r\n",
    "#             t_emb = t_emb + noise_t\n",
    "\n",
    "#         input = torch.cat([h_emb, r_emb, t_emb], dim=-1)\n",
    "#         score = self.score_net(input)\n",
    "#         return score\n",
    "\n",
    "# def build_model(config):\n",
    "#     # Load pre-trained RotatE model (embedding_model)\n",
    "#     embedding_model = RotatE(num_nodes=train_data.num_nodes, num_relations=train_data.num_edge_types, hidden_channels=50).to(config[\"device\"])\n",
    "#     embedding_model.load_state_dict(torch.load(\"../embedding_model/FB15k_237_RotatE_embedding_model_weights.pth\"))\n",
    "#     embedding_model.eval()\n",
    "\n",
    "#     # Get entity and relation embeddings\n",
    "#     with torch.no_grad():\n",
    "#         entity_embeddings_real = embedding_model.node_emb.weight.detach()\n",
    "#         entity_embeddings_im = embedding_model.node_emb_im.weight.detach()\n",
    "#         entity_embeddings = torch.cat([entity_embeddings_real, entity_embeddings_im], dim=-1)\n",
    "#         relation_embeddings = embedding_model.rel_emb.weight.detach()\n",
    "\n",
    "#     # Initialize score-based model\n",
    "#     score_model = ScoreModel(embedding_model, entity_embeddings.shape[-1], relation_embeddings.shape[-1]).to(config[\"device\"])\n",
    "\n",
    "\n",
    "#     # Create data loaders\n",
    "#     train_dataset = TensorDataset(train_data.edge_index[0], train_data.edge_type, train_data.edge_index[1])\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "#     val_dataset = TensorDataset(val_data.edge_index[0], val_data.edge_type, val_data.edge_index[1])\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "#     test_dataset = TensorDataset(test_data.edge_index[0], test_data.edge_type, test_data.edge_index[1])\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "#     # Initialize optimizer\n",
    "#     optimizer = Adam(score_model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "\n",
    "#     return score_model, optimizer, train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# # Define loss function\n",
    "# def denoising_score_matching_loss(score_model, h, r, t, noise_level):\n",
    "#     \"\"\"\n",
    "#     Denoising score-matching loss with noise-conditional score networks.\n",
    "#     \"\"\"\n",
    "#     true_score = score_model(h, r, t) #simply do not pass in the noise\n",
    "#     noisy_score = score_model(h, r, t, noise_level)\n",
    "#     return ((true_score - noisy_score) ** 2).mean()\n",
    "\n",
    "\n",
    "\n",
    "# def train_model(score_model, optimizer, train_loader, config):\n",
    "#     score_model.train()\n",
    "#     loss_epoch = 0\n",
    "#     for batch in train_loader:\n",
    "#         h, r, t = batch\n",
    "#         h, r, t = h.to(config[\"device\"]), r.to(config[\"device\"]), t.to(config[\"device\"])\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         noise_level = torch.rand(1) * config[\"noise_max\"]  # Sample a noise level for this batch\n",
    "#         loss = denoising_score_matching_loss(score_model, h, r, t, noise_level)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         loss_epoch += loss.item()\n",
    "\n",
    "#     return loss_epoch / len(train_loader)\n",
    "\n",
    "# def test_model(score_model, test_loader, config):\n",
    "#     score_model.eval()\n",
    "#     test_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_loader:\n",
    "#             h, r, t = batch\n",
    "#             h, r, t = h.to(config[\"device\"]), r.to(config[\"device\"]), t.to(config[\"device\"])\n",
    "#             loss = denoising_score_matching_loss(score_model, h, r, t, None)\n",
    "#             test_loss += loss.item()\n",
    "\n",
    "#     return test_loss / len(test_loader)\n",
    "\n",
    "# def compute_metrics(score_model, test_loader):\n",
    "#     \"\"\"\n",
    "#     Compute link prediction metrics: mean rank, mean reciprocal rank, and Hits@k.\n",
    "#     \"\"\"\n",
    "#     pass\n",
    "\n",
    "# def main(config):\n",
    "#     score_model, optimizer, train_loader, val_loader, test_loader = build_model(config)\n",
    "\n",
    "#     for epoch in range(config[\"num_epochs\"]):\n",
    "#         train_loss = train_model(score_model, optimizer, train_loader, config)\n",
    "#         test_loss = test_model(score_model, test_loader, config)\n",
    "#         print(f\"\\n\\nEpoch {epoch + 1}\\nTrain Loss: {train_loss:.20f}\\nTest Loss: {test_loss:.20f}\")\n",
    "\n",
    "#     compute_metrics(score_model, test_loader)\n",
    "\n",
    "# config = {\n",
    "#     \"batch_size\": 64,\n",
    "#     \"num_epochs\": 2,\n",
    "#     \"num_steps\": 1000,\n",
    "#     \"lr\": 1e-4,\n",
    "#     \"device\": device,\n",
    "#     \"noise_max\": 0.5  # 20% of the original data values\n",
    "# }\n",
    "\n",
    "# main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim import Adam\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from torch_geometric.nn import RotatE\n",
    "# from torch_geometric.datasets import FB15k_237\n",
    "# import sys\n",
    "# import os\n",
    "# import os.path as osp\n",
    "# import math\n",
    "# import numpy as np\n",
    "# from icecream import ic\n",
    "\n",
    "# # Set up paths\n",
    "# parent_dir = os.path.abspath(os.pardir)\n",
    "# sys.path.append(parent_dir)\n",
    "# data_path = osp.join(parent_dir, \"data\", \"FB15k_237\")\n",
    "\n",
    "# # Load dataset\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# train_data = FB15k_237(data_path, split=\"train\")[0].to(device)\n",
    "# val_data = FB15k_237(data_path, split=\"val\")[0].to(device)\n",
    "# test_data = FB15k_237(data_path, split=\"test\")[0].to(device)\n",
    "\n",
    "\n",
    "# # Define cosine noise schedule\n",
    "# def cosine_beta_schedule(timesteps, s=0.008):\n",
    "#     \"\"\"\n",
    "#     Cosine schedule as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "#     \"\"\"\n",
    "#     steps = timesteps + 1\n",
    "#     x = torch.linspace(0, timesteps, steps)\n",
    "#     alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "#     alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "#     betas = 1 - alphas_cumprod[1:] / alphas_cumprod[:-1]\n",
    "#     return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "# def diffusion_process(e, r, t, beta, device):\n",
    "#     \"\"\"\n",
    "#     Diffusion process for triplets.\n",
    "#     e, r, t: entity and relation embeddings\n",
    "#     beta: noise level\n",
    "#     \"\"\"\n",
    "#     noise_e = torch.randn_like(e, device=device)\n",
    "#     noise_r = torch.randn_like(r, device=device)\n",
    "#     noise_t = torch.randn_like(t, device=device)\n",
    "    \n",
    "#     noisy_e = (1 - beta) ** 0.5 * e + beta ** 0.5 * noise_e\n",
    "#     noisy_r = (1 - beta) ** 0.5 * r + beta ** 0.5 * noise_r\n",
    "#     noisy_t = (1 - beta) ** 0.5 * t + beta ** 0.5 * noise_t\n",
    "    \n",
    "#     return noisy_e, noisy_r, noisy_t \n",
    "\n",
    "\n",
    "# class ScoreModel(nn.Module):\n",
    "#     def __init__(self, embedding_model, embedding_dim, relation_dim, time_emb_dim):\n",
    "#         super(ScoreModel, self).__init__()\n",
    "#         self.embedding_model = embedding_model\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.relation_dim = relation_dim\n",
    "#         self.time_emb = nn.Linear(1, time_emb_dim)\n",
    "#         self.score_net = nn.Sequential(\n",
    "#             nn.Linear(embedding_dim + relation_dim + time_emb_dim, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, embedding_dim + relation_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, h, r, t, t_emb):\n",
    "#         h_emb = self.embedding_model.node_emb(h)\n",
    "#         r_emb = self.embedding_model.rel_emb(r)\n",
    "#         t_emb = self.embedding_model.node_emb(t)\n",
    "#         time_emb = self.time_emb(t_emb)\n",
    "#         input = torch.cat([h_emb, r_emb, t_emb, time_emb], dim=-1)\n",
    "#         score = self.score_net(input)\n",
    "#         return score\n",
    "\n",
    "# def build_model(config):\n",
    "#     # Load pre-trained RotatE model (embedding_model)\n",
    "#     embedding_model = RotatE(num_nodes=train_data.num_nodes, num_relations=train_data.num_edge_types, hidden_channels=50).to(config[\"device\"])\n",
    "#     embedding_model.load_state_dict(torch.load(\"../embedding_model/FB15k_237_RotatE_embedding_model_weights.pth\"))\n",
    "#     embedding_model.eval()\n",
    "\n",
    "#     # Get entity and relation embeddings\n",
    "#     with torch.no_grad():\n",
    "#         entity_embeddings_real = embedding_model.node_emb.weight.detach()\n",
    "#         entity_embeddings_im = embedding_model.node_emb_im.weight.detach()\n",
    "#         entity_embeddings = torch.cat([entity_embeddings_real, entity_embeddings_im], dim=-1)\n",
    "#         relation_embeddings = embedding_model.rel_emb.weight.detach()\n",
    "\n",
    "#     # Initialize score-based model\n",
    "#     score_model = ScoreModel(embedding_model, entity_embeddings.shape[-1], relation_embeddings.shape[-1]).to(config[\"device\"])\n",
    "\n",
    "\n",
    "#     # Create data loaders\n",
    "#     train_dataset = TensorDataset(train_data.edge_index[0], train_data.edge_type, train_data.edge_index[1])\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "#     val_dataset = TensorDataset(val_data.edge_index[0], val_data.edge_type, val_data.edge_index[1])\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "#     test_dataset = TensorDataset(test_data.edge_index[0], test_data.edge_type, test_data.edge_index[1])\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "#     # Initialize optimizer\n",
    "#     optimizer = Adam(score_model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "\n",
    "#     return score_model, optimizer, train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# def denoising_score_matching_loss(score_model, h, r, t, noisy_h, noisy_r, noisy_t, t_emb):\n",
    "#     \"\"\"\n",
    "#     Denoising score-matching loss for triplets.\n",
    "#     \"\"\"\n",
    "#     true_score = score_model(h, r, t, t_emb)\n",
    "#     noisy_score = score_model(noisy_h, noisy_r, noisy_t, t_emb)\n",
    "#     return ((true_score - noisy_score) ** 2).mean()\n",
    "\n",
    "\n",
    "\n",
    "# def train_model(score_model, optimizer, train_loader, config):\n",
    "#     score_model.train()\n",
    "#     loss_epoch = 0\n",
    "#     for batch in train_loader:\n",
    "#         h, r, t = batch\n",
    "#         h, r, t = h.to(config[\"device\"]), r.to(config[\"device\"]), t.to(config[\"device\"])\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         beta = torch.rand(1, device=config[\"device\"]) * config[\"noise_max\"]  # Sample a noise level for this batch\n",
    "#         t_emb = torch.tensor([beta], dtype=torch.float, device=config[\"device\"])\n",
    "#         noisy_h, noisy_r, noisy_t = diffusion_process(h, r, t, beta, config[\"device\"])\n",
    "#         loss = denoising_score_matching_loss(score_model, h, r, t, noisy_h, noisy_r, noisy_t, t_emb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         loss_epoch += loss.item()\n",
    "\n",
    "#     return loss_epoch / len(train_loader)\n",
    "\n",
    "# def test_model(score_model, test_loader, config):\n",
    "#     score_model.eval()\n",
    "#     test_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_loader:\n",
    "#             h, r, t = batch\n",
    "#             h, r, t = h.to(config[\"device\"]), r.to(config[\"device\"]), t.to(config[\"device\"])\n",
    "#             t_emb = torch.tensor([0.0], dtype=torch.float, device=config[\"device\"])\n",
    "#             loss = denoising_score_matching_loss(score_model, h, r, t, h, r, t, t_emb)\n",
    "#             test_loss += loss.item()\n",
    "\n",
    "#     return test_loss / len(test_loader)\n",
    "\n",
    "# def compute_metrics(score_model, test_loader, config, k=10):\n",
    "#     score_model.eval()\n",
    "#     mean_rank = []\n",
    "#     mean_reciprocal_rank = []\n",
    "#     hits_at_k = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_loader:\n",
    "#             h, r, t = batch\n",
    "#             h, r, t = h.to(config[\"device\"]), r.to(config[\"device\"]), t.to(config[\"device\"])\n",
    "#             t_emb = torch.tensor([0.0], dtype=torch.float, device=config[\"device\"])\n",
    "#             scores = score_model(h, r, t, t_emb)\n",
    "            \n",
    "#             # Compute metrics\n",
    "#             ranks = torch.argsort(scores, dim=-1, descending=True) + 1\n",
    "#             mean_rank.append(ranks.float().mean().item())\n",
    "#             mean_reciprocal_rank.append((1 / ranks.float()).mean().item())\n",
    "#             hits_at_k.append((ranks <= k).float().mean().item())\n",
    "    \n",
    "#     print(f\"Mean Rank: {sum(mean_rank) / len(mean_rank):.4f}\")\n",
    "#     print(f\"Mean Reciprocal Rank: {sum(mean_reciprocal_rank) / len(mean_reciprocal_rank):.4f}\")\n",
    "#     print(f\"Hits@{k}: {sum(hits_at_k) / len(hits_at_k):.4f}\")\n",
    "\n",
    "# def link_prediction(score_model, test_loader, config):\n",
    "#     score_model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_loader:\n",
    "#             h, r, t = batch\n",
    "#             h, r, t = h.to(config[\"device\"]), r.to(config[\"device\"]), t.to(config[\"device\"])\n",
    "#             t_emb = torch.tensor([0.0], dtype=torch.float, device=config[\"device\"])\n",
    "#             scores = score_model(h, r, t, t_emb)\n",
    "            \n",
    "#             # Compute metrics or perform ranking\n",
    "#             # ...\n",
    "\n",
    "# def main(config):\n",
    "#     score_model, optimizer, train_loader, val_loader, test_loader = build_model(config)\n",
    "\n",
    "#     for epoch in range(config[\"num_epochs\"]):\n",
    "#         train_loss = train_model(score_model, optimizer, train_loader, config)\n",
    "#         test_loss = test_model(score_model, test_loader, config)\n",
    "#         print(f\"\\n\\nEpoch {epoch + 1}\\nTrain Loss: {train_loss:.20f}\\nTest Loss: {test_loss:.20f}\")\n",
    "\n",
    "#     compute_metrics(score_model, test_loader)\n",
    "\n",
    "# config = {\n",
    "#     \"batch_size\": 64,\n",
    "#     \"num_epochs\": 2,\n",
    "#     \"num_steps\": 1000,\n",
    "#     \"lr\": 1e-4,\n",
    "#     \"device\": device,\n",
    "#     \"noise_max\": 0.5  # 20% of the original data values\n",
    "# }\n",
    "\n",
    "# main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blaineh2/anaconda3/envs/ScoreMatchingDiffKG/lib/python3.9/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1\n",
      "Train Loss: 21.83444665404613260762\n",
      "Test Loss: 12.62340334191918422846\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Train Loss: 18.41383945504562547058\n",
      "Test Loss: 10.42930970609188001674\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "odeint() got an unexpected keyword argument 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 227\u001b[0m\n\u001b[1;32m    217\u001b[0m     compute_metrics(score_model, test_loader, config)\n\u001b[1;32m    219\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: device,\n\u001b[1;32m    225\u001b[0m }\n\u001b[0;32m--> 227\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 217\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    214\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m test_model(score_model, test_loader, config)\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.20f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.20f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 217\u001b[0m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 182\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(score_model, test_loader, config)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(h\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    181\u001b[0m     t_init \u001b[38;5;241m=\u001b[39m score_model\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;241m.\u001b[39mnode_emb(candidate_t[i])\n\u001b[0;32m--> 182\u001b[0m     refined_candidate \u001b[38;5;241m=\u001b[39m \u001b[43mreverse_sde\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     refined_candidates\u001b[38;5;241m.\u001b[39mappend(refined_candidate)\n\u001b[1;32m    184\u001b[0m refined_candidates \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(refined_candidates, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 78\u001b[0m, in \u001b[0;36mreverse_sde\u001b[0;34m(h, r, t_init, score_model, config)\u001b[0m\n\u001b[1;32m     76\u001b[0m t_steps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     77\u001b[0m additional_args \u001b[38;5;241m=\u001b[39m (h, r, score_model)  \u001b[38;5;66;03m# Group additional arguments in a tuple\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m t_emb \u001b[38;5;241m=\u001b[39m \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43msde_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t_emb\n",
      "\u001b[0;31mTypeError\u001b[0m: odeint() got an unexpected keyword argument 'args'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.nn import RotatE\n",
    "from torch_geometric.datasets import FB15k_237\n",
    "import sys\n",
    "import os\n",
    "import os.path as osp\n",
    "import math\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "from torchdiffeq import odeint  # library for solving differential equations\n",
    "\n",
    "# Set up paths\n",
    "parent_dir = os.path.abspath(os.pardir)\n",
    "sys.path.append(parent_dir)\n",
    "data_path = osp.join(parent_dir, \"data\", \"FB15k_237\")\n",
    "\n",
    "# Load dataset\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_data = FB15k_237(data_path, split=\"train\")[0].to(device)\n",
    "val_data = FB15k_237(data_path, split=\"val\")[0].to(device)\n",
    "test_data = FB15k_237(data_path, split=\"test\")[0].to(device)\n",
    "\n",
    "\n",
    "class ScoreModel(nn.Module):\n",
    "    def __init__(self, embedding_model, embedding_dim, relation_dim, config):\n",
    "        super(ScoreModel, self).__init__()\n",
    "        self.embedding_model = embedding_model\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.relation_dim = relation_dim\n",
    "        self.config = config  # store config for hyperparameters\n",
    "\n",
    "        self.score_net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + relation_dim, 512, dtype=torch.float),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512, dtype=torch.float),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1, dtype=torch.float),\n",
    "        )\n",
    "\n",
    "    def forward(self, h, r, t, timestep=None):\n",
    "        h_emb = self.embedding_model.node_emb(h)\n",
    "        r_emb = self.embedding_model.rel_emb(r)\n",
    "        t_emb = self.embedding_model.node_emb(t)\n",
    "\n",
    "        # Implement your desired distance measure here (e.g., L2 distance)\n",
    "        distance = torch.linalg.norm(h_emb + r_emb - t_emb, dim=-1)\n",
    "\n",
    "        # Gradually increase the weight of the distance term during SDE steps\n",
    "        weight = 0.0  # No weight increase if timestep is None\n",
    "        if timestep is not None:\n",
    "            def sigmoid(x):\n",
    "                return 1 / (1 + math.exp(-x))\n",
    "            weight = sigmoid(timestep / (self.config[\"num_steps\"] - 1))\n",
    "        score = weight * distance\n",
    "        return score\n",
    "\n",
    "\n",
    "def sde_func(t, y, h, r, score_model):\n",
    "    \"\"\"\n",
    "    Function for the reverse SDE process.\n",
    "    \"\"\"\n",
    "    t_emb = y.clone()  # current state of the target entity embedding\n",
    "    with torch.no_grad():\n",
    "        score_grad = torch.autograd.grad(score_model(h, r, t_emb, t), [t_emb])[0]\n",
    "    return -score_grad  # Update in the direction that minimizes the score\n",
    "\n",
    "\n",
    "def reverse_sde(h, r, t_init, score_model, config):\n",
    "    \"\"\"\n",
    "    Perform reverse SDE to refine the target entity embedding.\n",
    "    \"\"\"\n",
    "    t_steps = torch.linspace(0, 1.0, config[\"num_steps\"]).to(config[\"device\"])\n",
    "    t_emb = odeint(sde_func, t_init, t_steps, h=h, r=r, score_model=score_model)[-1, :]\n",
    "    return t_emb\n",
    "\n",
    "\n",
    "def build_model(config):\n",
    "    # Load pre-trained RotatE model (embedding_model)\n",
    "    embedding_model = RotatE(num_nodes=train_data.num_nodes, num_relations=train_data.num_edge_types, hidden_channels=50).to(config[\"device\"])\n",
    "    embedding_model.load_state_dict(torch.load(\"../embedding_model/FB15k_237_RotatE_embedding_model_weights.pth\"))\n",
    "    embedding_model.eval()\n",
    "\n",
    "    # Get entity and relation embeddings\n",
    "    with torch.no_grad():\n",
    "        entity_embeddings_real = embedding_model.node_emb.weight.detach()\n",
    "        entity_embeddings_im = embedding_model.node_emb_im.weight.detach()\n",
    "        entity_embeddings = torch.cat([entity_embeddings_real, entity_embeddings_im], dim=-1)\n",
    "        relation_embeddings = embedding_model.rel_emb.weight.detach()\n",
    "\n",
    "    # Initialize score-based model\n",
    "    score_model = ScoreModel(embedding_model, entity_embeddings.shape[-1], relation_embeddings.shape[-1], config).to(config[\"device\"])\n",
    "\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(train_data.edge_index[0], train_data.edge_type, train_data.edge_index[1])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(val_data.edge_index[0], val_data.edge_type, val_data.edge_index[1])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    test_dataset = TensorDataset(test_data.edge_index[0], test_data.edge_type, test_data.edge_index[1])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = Adam(score_model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "\n",
    "    return score_model, optimizer, train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "def denoising_score_matching_loss(score_model, h, r, t, timestep):\n",
    "    \"\"\"\n",
    "    Denoising score-matching loss with noise-conditional score networks.\n",
    "    \"\"\"\n",
    "    true_score = score_model(h, r, t)  # simply do not pass in the timestep\n",
    "    noisy_score = score_model(h, r, t, timestep)\n",
    "    return ((true_score - noisy_score) ** 2).mean()\n",
    "\n",
    "\n",
    "def train_model(score_model, optimizer, train_loader, config):\n",
    "    score_model.train()\n",
    "    loss_epoch = 0\n",
    "    for batch in train_loader:\n",
    "        h, r, t = batch\n",
    "        h, r, t = h.to(config[\"device\"]), r.to(config[\"device\"]), t.to(config[\"device\"])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for timestep in range(config[\"num_steps\"]):\n",
    "            loss = denoising_score_matching_loss(score_model, h, r, t, timestep)\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "    return loss_epoch / len(train_loader)\n",
    "\n",
    "\n",
    "\n",
    "def test_model(score_model, test_loader, config):\n",
    "    score_model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            h, r, t = batch\n",
    "            h, r, t = h.to(config[\"device\"]), r.to(config[\"device\"]), t.to(config[\"device\"])\n",
    "            for timestep in range(config[\"num_steps\"]):\n",
    "                loss = denoising_score_matching_loss(score_model, h, r, t, timestep)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "    return test_loss / (len(test_loader) * config[\"num_steps\"])  # Average across steps\n",
    "\n",
    "\n",
    "def compute_metrics(score_model, test_loader, config):\n",
    "    \"\"\"\n",
    "    Compute link prediction metrics: mean rank, mean reciprocal rank, and Hits@k.\n",
    "    \"\"\"\n",
    "    score_model.eval()\n",
    "    mean_rank = 0.0\n",
    "    mean_reciprocal_rank = 0.0\n",
    "    hits_at_10 = 0\n",
    "    hits_at_3 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            h, r, t = batch\n",
    "            h, r = h.to(config[\"device\"]), r.to(config[\"device\"])\n",
    "\n",
    "            # Sample a set of candidate target entities (t')\n",
    "            num_candidates = 100  # You can adjust this value\n",
    "            candidate_t = torch.randint(0, score_model.embedding_model.num_nodes, size=(h.shape[0], num_candidates)).to(config[\"device\"])\n",
    "\n",
    "            # Refine candidate embeddings using reverse SDE\n",
    "            refined_candidates = []\n",
    "            for i in range(h.shape[0]):\n",
    "                t_init = score_model.embedding_model.node_emb(candidate_t[i])\n",
    "                refined_candidate = reverse_sde(h[i], r[i], t_init, score_model, config)\n",
    "                refined_candidates.append(refined_candidate)\n",
    "            refined_candidates = torch.stack(refined_candidates, dim=0)\n",
    "\n",
    "            # Compute scores for all candidates (including ground truth)\n",
    "            all_t = torch.cat([t.unsqueeze(1), refined_candidates], dim=1)\n",
    "            scores = score_model(h.repeat(1, num_candidates + 1), r.repeat(1, num_candidates + 1), all_t, torch.ones(h.shape[0], num_candidates + 1) * config[\"num_steps\"]).squeeze(2)\n",
    "\n",
    "            # Evaluate link prediction metrics\n",
    "            for i in range(h.shape[0]):\n",
    "                ground_truth_rank = (scores[i] == scores[i].max()).nonzero(as_tuple=True)[0].item()\n",
    "                mean_rank += 1 + ground_truth_rank\n",
    "                mean_reciprocal_rank += 1.0 / (1 + ground_truth_rank)\n",
    "                hits_at_10 += (scores[i][:10] == scores[i].max()).any().item()\n",
    "                hits_at_3 += (scores[i][:3] == scores[i].max()).any().item()\n",
    "\n",
    "    mean_rank /= len(test_loader.dataset)\n",
    "    mean_reciprocal_rank /= len(test_loader.dataset)\n",
    "    hits_at_10 /= len(test_loader.dataset)\n",
    "    hits_at_3 /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"\\nMean Rank: {mean_rank:.4f}\")\n",
    "    print(f\"Mean Reciprocal Rank: {mean_reciprocal_rank:.4f}\")\n",
    "    print(f\"Hits@10: {hits_at_10:.4f}\")\n",
    "    print(f\"Hits@3: {hits_at_3:.4f}\")\n",
    "\n",
    "\n",
    "def main(config):\n",
    "    score_model, optimizer, train_loader, val_loader, test_loader = build_model(config)\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train_model(score_model, optimizer, train_loader, config)\n",
    "        test_loss = test_model(score_model, test_loader, config)\n",
    "        print(f\"\\n\\nEpoch {epoch + 1}\\nTrain Loss: {train_loss:.20f}\\nTest Loss: {test_loss:.20f}\")\n",
    "\n",
    "    compute_metrics(score_model, test_loader, config)\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 1,\n",
    "    \"num_steps\": 20,  # Adjust based on your desired SDE iterations\n",
    "    \"lr\": 1e-4,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "main(config)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ScoreMatchingDiffKG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
