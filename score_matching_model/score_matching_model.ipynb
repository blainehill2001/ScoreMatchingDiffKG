{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Blaine Hill\n",
    "\n",
    "In this notebook, we train a score matching model as described by [Yang Song](https://yang-song.net/blog/2021/score/)\n",
    "\n",
    "\n",
    "$$s_\\theta(x) = \\nabla_x \\log p_\\theta(x) = -\\nabla_x f_\\theta(x) - \\nabla_x \\log Z_\\theta = -\\nabla_x f_\\theta(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbthill1\u001b[0m (\u001b[33muiuc_idealab_2024\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/blaineh2/anaconda3/envs/ScoreMatchingDiffKG/lib/python3.9/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"score_matching_model.ipynb\"\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "# wandb.login(relogin=True)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.nn import RotatE\n",
    "from torch_geometric.datasets import FB15k_237\n",
    "import sys\n",
    "import os\n",
    "import os.path as osp\n",
    "import math\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "dataset_name = \"FB15k_237\"\n",
    "\n",
    "parent_dir = os.path.abspath(os.pardir)\n",
    "sys.path.append(parent_dir)\n",
    "data_path = osp.join(parent_dir, \"data\", dataset_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "train_data = FB15k_237(data_path, split=\"train\")[0].to(device)\n",
    "val_data = FB15k_237(data_path, split=\"val\")[0].to(device)\n",
    "test_data = FB15k_237(data_path, split=\"test\")[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sweep = True\n",
    "if run_sweep:\n",
    "    with open(\"sweep_config.yaml\", \"r\") as file:\n",
    "        sweep_config = yaml.safe_load(file)\n",
    "else:\n",
    "    config = {\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\": 100,\n",
    "        \"num_steps\": 20,  # Adjust based on your desired SDE iterations\n",
    "        \"lr\": 1e-4,\n",
    "        \"device\": device,\n",
    "        \"k\": 10,\n",
    "        \"similarity_metric\": \"cosine\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreModel(nn.Module):\n",
    "    def __init__(self, embedding_model, node_emb_dim, rel_emb_dim, config):\n",
    "        super(ScoreModel, self).__init__()\n",
    "        self.config = config  # store config for hyperparameters\n",
    "        self.embedding_model = embedding_model\n",
    "        self.node_emb_dim = node_emb_dim\n",
    "        self.rel_emb_dim = rel_emb_dim\n",
    "        self.score_net = nn.Sequential(\n",
    "            nn.Linear(node_emb_dim + rel_emb_dim, 512, dtype=torch.float),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512, dtype=torch.float),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1, dtype=torch.float),\n",
    "        )\n",
    "\n",
    "    def forward(self, h_emb, r_emb, t_emb, timestep=None):\n",
    "        # Implement your desired distance measure here (e.g., L2 distance)\n",
    "        distance = torch.linalg.norm(h_emb + r_emb - t_emb, dim=-1)\n",
    "\n",
    "        # Gradually increase the weight of the distance term during SDE steps\n",
    "        weight = 0.0  # No weight increase if timestep is None\n",
    "        if timestep is not None:\n",
    "\n",
    "            def sigmoid(x):\n",
    "                return 1 / (1 + math.exp(-x))\n",
    "\n",
    "            weight = sigmoid(timestep / (self.config[\"num_steps\"] - 1))\n",
    "        score = weight * distance\n",
    "        return score\n",
    "\n",
    "\n",
    "def build_model(config):\n",
    "    embedding_model = RotatE(\n",
    "        num_nodes=train_data.num_nodes,\n",
    "        num_relations=train_data.num_edge_types,\n",
    "        hidden_channels=50,\n",
    "    ).to(device)\n",
    "    embedding_model.load_state_dict(\n",
    "        torch.load(\"../embedding_model/FB15k_237_RotatE_embedding_model_weights.pth\")\n",
    "    )\n",
    "    embedding_model.eval()\n",
    "\n",
    "    # Get entity and relation embeddings\n",
    "    with torch.no_grad():\n",
    "        entity_embeddings_real = embedding_model.node_emb.weight.detach()\n",
    "        entity_embeddings_im = embedding_model.node_emb_im.weight.detach()\n",
    "        entity_embeddings = torch.cat([entity_embeddings_real, entity_embeddings_im], dim=-1)\n",
    "        relation_embeddings = embedding_model.rel_emb.weight.detach()\n",
    "\n",
    "    score_model = ScoreModel(\n",
    "        embedding_model, entity_embeddings.shape[-1], relation_embeddings.shape[-1], config\n",
    "    ).to(device)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(\n",
    "        train_data.edge_index[0], train_data.edge_type, train_data.edge_index[1]\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(\n",
    "        val_data.edge_index[0], val_data.edge_type, val_data.edge_index[1]\n",
    "    )\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    test_dataset = TensorDataset(\n",
    "        test_data.edge_index[0], test_data.edge_type, test_data.edge_index[1]\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    optimizer = Adam(score_model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    return score_model, optimizer, train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def denoising_score_matching_loss(score_model, h, r, t, timestep):\n",
    "    \"\"\"\n",
    "    Denoising score-matching loss with noise-conditional score networks.\n",
    "    \"\"\"\n",
    "    h_emb, r_emb, t_emb = (\n",
    "        score_model.embedding_model.node_emb(h),\n",
    "        score_model.embedding_model.rel_emb(r),\n",
    "        score_model.embedding_model.node_emb(t),\n",
    "    )\n",
    "    true_score = score_model(h_emb, r_emb, t_emb)  # simply do not pass in the timestep\n",
    "    noisy_score = score_model(h_emb, r_emb, t_emb, timestep)\n",
    "    return ((true_score - noisy_score) ** 2).mean()\n",
    "\n",
    "\n",
    "def train_model(score_model, optimizer, train_loader, config):\n",
    "    score_model.train()\n",
    "    loss_epoch = 0\n",
    "    for batch in train_loader:\n",
    "        h, r, t = batch\n",
    "        h, r, t = h.to(device), r.to(device), t.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for timestep in range(config[\"num_steps\"]):\n",
    "            loss = denoising_score_matching_loss(score_model, h, r, t, timestep)\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "    return loss_epoch / len(train_loader)\n",
    "\n",
    "\n",
    "def test_model(score_model, test_loader, config):\n",
    "    score_model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            h, r, t = batch\n",
    "            h, r, t = h.to(device), r.to(device), t.to(device)\n",
    "            for timestep in range(config[\"num_steps\"]):\n",
    "                loss = denoising_score_matching_loss(score_model, h, r, t, timestep)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "    return test_loss / (len(test_loader) * config[\"num_steps\"])  # Average across steps\n",
    "\n",
    "\n",
    "def reverse_sde(h_emb, t_emb, score_model, config):\n",
    "    \"\"\"\n",
    "    Refine the relation embedding using reverse SDE for better link prediction.\n",
    "    h_emb: Embedding of the head entity.\n",
    "    t_emb: Embedding of the tail entity.\n",
    "    score_model: The trained score model.\n",
    "    config: Configuration dictionary.\n",
    "    \"\"\"\n",
    "    # Initialize with random noise\n",
    "    r_emb = torch.randn(score_model.rel_emb_dim, device=device)\n",
    "\n",
    "    # Define the time steps for the reverse SDE process\n",
    "    t_steps = torch.linspace(config[\"num_steps\"] - 1, 0, config[\"num_steps\"]).to(device)\n",
    "\n",
    "    # Define the SDE function for the reverse process\n",
    "    def sde_func(t, r_emb):\n",
    "        with torch.enable_grad():\n",
    "            r_emb.requires_grad_(True)\n",
    "            score = score_model(h_emb, r_emb, t_emb, t)\n",
    "            grad_r_emb = torch.autograd.grad(score.sum(), r_emb, create_graph=True)[0]\n",
    "        return -grad_r_emb  # Reverse direction\n",
    "\n",
    "    # Perform the reverse SDE integration\n",
    "    with torch.no_grad():\n",
    "        for t in reversed(t_steps):\n",
    "            r_emb = sde_func(t, r_emb)\n",
    "\n",
    "    return r_emb\n",
    "\n",
    "\n",
    "def compute_metrics(score_model, test_loader, config):\n",
    "    all_ranks = []\n",
    "    all_hits_at_k = []\n",
    "\n",
    "    # Get all relation embeddings from the model\n",
    "    all_relations = score_model.embedding_model.rel_emb.weight.detach()\n",
    "\n",
    "    for batch in test_loader:\n",
    "        h, true_r, t = batch\n",
    "        h, true_r, t = h.to(device), true_r.to(device), t.to(device)\n",
    "\n",
    "        h_emb = score_model.embedding_model.node_emb(h)\n",
    "        t_emb = score_model.embedding_model.node_emb(t)\n",
    "        # Assuming reverse_sde function returns refined relation embeddings\n",
    "        refined_r_emb = reverse_sde(h_emb, t_emb, score_model, config)\n",
    "        if config[\"similarity_metric\"] == \"cosine\":\n",
    "            # Calculate cosine similarity\n",
    "            similarity = F.cosine_similarity(refined_r_emb.unsqueeze(0), all_relations, dim=1)\n",
    "            # Since cosine similarity is higher for closer vectors, we use largest=True\n",
    "            values, indices = similarity.topk(config[\"k\"], largest=True)\n",
    "        elif config[\"similarity_metric\"] == \"l2\":  # Euclidean distance\n",
    "            # Expand dimensions to broadcast and calculate pairwise Euclidean distance\n",
    "            dist = torch.norm(all_relations - refined_r_emb.unsqueeze(0), dim=2, p=2)\n",
    "\n",
    "            # For Euclidean distance, closer vectors have smaller distances, so largest=False\n",
    "            values, indices = dist.topk(config[\"k\"], largest=False)\n",
    "\n",
    "        # Calculate metrics\n",
    "        # Check if true relation is within the top K predictions\n",
    "        hits_at_k = (indices == true_r.unsqueeze(1)).any(dim=1).float().mean().item()\n",
    "        all_hits_at_k.append(hits_at_k)\n",
    "\n",
    "        # Calculate rank of the true relation\n",
    "        for i in range(indices.size(0)):\n",
    "            true_relation_idx = (indices[i] == true_r[i]).nonzero(as_tuple=True)[0]\n",
    "            rank = true_relation_idx.item() + 1 if true_relation_idx.numel() > 0 else 0\n",
    "            all_ranks.append(rank)\n",
    "\n",
    "    mean_rank = np.mean(all_ranks)\n",
    "    mrr = np.mean([1.0 / rank for rank in all_ranks if rank > 0])\n",
    "    hits_at_k = np.mean(all_hits_at_k)\n",
    "\n",
    "    return mean_rank, mrr, hits_at_k\n",
    "\n",
    "\n",
    "def main(config=None, verbose=False):\n",
    "    with wandb.init(\n",
    "        project=f\"ScoreMatchingDiffKG_Embedding\",\n",
    "        name=f\"{dataset_name}_score_matching_model {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        config=config if config is not None else {},\n",
    "    ):\n",
    "        config = wandb.config\n",
    "        model, optimizer, train_loader, val_loader, test_loader = build_model(config)\n",
    "        wandb.watch(model)\n",
    "        for epoch in range(config.epochs):\n",
    "            loss = train_model(model, optimizer, train_loader, config)\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {epoch:03d}, Train Loss: {loss:.4f}\")\n",
    "            train_metrics = {\"train_epoch\": epoch, \"train_loss\": loss}\n",
    "            if epoch % 10 == 0 and epoch > 0:\n",
    "                loss = test_model(model, test_loader, config)\n",
    "                mean_rank, mrr, hits_at_k = compute_metrics(model, test_loader, config)\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f\"Test Mean Rank: {mean_rank:.2f}, Test Mean Reciprocal Rank: {mrr:.2f}, Test Hits@K: {hits_at_k:.4f}\"\n",
    "                    )\n",
    "                test_metrics = {\n",
    "                    \"test_loss\": loss,\n",
    "                    \"test_mean_rank\": mean_rank,\n",
    "                    \"test_mrr\": mrr,\n",
    "                    \"test_hits_at_k\": hits_at_k,\n",
    "                }\n",
    "            # log to wandb\n",
    "            wandb.log(\n",
    "                {**train_metrics, **test_metrics}\n",
    "                if \"test_metrics\" in locals()\n",
    "                else {**train_metrics}\n",
    "            )\n",
    "\n",
    "        # Save the trained model\n",
    "        path = osp.join(os.getcwd(), f\"{dataset_name}_score_matching_model_weights.pth\")\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "        # Fetch a batch from train_loader\n",
    "        for batch in train_loader:\n",
    "            # Assuming batch contains head_index, rel_type, tail_index, and possibly other data\n",
    "            head_index, rel_type, tail_index = batch\n",
    "            break  # Only need one batch for this purpose\n",
    "\n",
    "        # Use the fetched batch to provide dummy inputs for the export\n",
    "        # Ensure these variables are moved to the same device as model if necessary\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            (head_index, rel_type, tail_index),  # Use actual data as dummy inputs\n",
    "            f\"{dataset_name}_score_matching_model_weights.onnx\",\n",
    "            opset_version=11,\n",
    "            do_constant_folding=True,\n",
    "            input_names=[\n",
    "                \"head_index\",\n",
    "                \"rel_type\",\n",
    "                \"tail_index\",\n",
    "            ],  # Adjust input names as needed\n",
    "            dynamic_axes={\n",
    "                \"head_index\": {0: \"batch_size\"},\n",
    "                \"rel_type\": {0: \"batch_size\"},\n",
    "                \"tail_index\": {0: \"batch_size\"},\n",
    "            },\n",
    "        )\n",
    "        wandb.save(f\"{dataset_name}_score_matching_model_weights.onnx\")\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. lr uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: afxexxqn\n",
      "Sweep URL: https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Score_Sweep/sweeps/afxexxqn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3ahyxk10 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 400\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tk: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.005338044831327\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_steps: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trel_emb_dim: 150\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsimilarity_metric: l2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/blaineh2/ScoreMatchingDiffKG/score_matching_model/wandb/run-20240421_231721-3ahyxk10</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Score_Sweep/runs/3ahyxk10' target=\"_blank\">FB15k_237_score_matching_model 2024-04-21 23:17:21</a></strong> to <a href='https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Score_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Score_Sweep/sweeps/afxexxqn' target=\"_blank\">https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Score_Sweep/sweeps/afxexxqn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Score_Sweep' target=\"_blank\">https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Score_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Score_Sweep/sweeps/afxexxqn' target=\"_blank\">https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Score_Sweep/sweeps/afxexxqn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Score_Sweep/runs/3ahyxk10' target=\"_blank\">https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Score_Sweep/runs/3ahyxk10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if run_sweep:\n",
    "\n",
    "    with open(\"sweep_config.yaml\", \"r\") as file:\n",
    "        sweep_config = yaml.safe_load(file)\n",
    "\n",
    "    sweep_id = wandb.sweep(project=f\"ScoreMatchingDiffKG_Score_Sweep\", sweep=sweep_config)\n",
    "\n",
    "    wandb.agent(sweep_id, function=main)\n",
    "else:\n",
    "    model = main(config, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ScoreMatchingDiffKG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
