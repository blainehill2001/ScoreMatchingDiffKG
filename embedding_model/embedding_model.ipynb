{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph Embedding System Overview\n",
    "\n",
    "In this notebook, we program out how to embed a KG such as FB15k_237 using a KG Embedding model such as RotatE. The weights are saved under at the top of the project directory under trained_embedding_models/embedding_model_weights.pth \n",
    "\n",
    "##### System Components\n",
    "1. **Model Training**: The system includes functions like `train_model` and `test_model` for training and evaluating the model on validation or test datasets.\n",
    "2. **Hyperparameter Sweep**: You can choose to run a hyperparameter sweep by setting `run_sweep=True` in the configuration.\n",
    "3. **Main Function**: The `main()` function serves as the core function for training the model, managing the training process, and saving the best model based on validation loss improvement.\n",
    "\n",
    "##### Instructions\n",
    "- To run a hyperparameter sweep, set `run_sweep=True`. This will trigger a sweep to find the best hyperparameters.\n",
    "- Ensure you are logged into [Weights and Biases](https://wandb.ai/site) using `wandb.login()` before running the system.\n",
    "- Monitor the training progress and validation loss improvement to save the best model during training.\n",
    "- Use the provided functions like `train_model` and `test_model` to train and evaluate the model effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbthill1\u001b[0m (\u001b[33muiuc_idealab_2024\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/blaineh2/anaconda3/envs/ScoreMatchingDiffKG/lib/python3.11/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from icecream import ic\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.nn import Module\n",
    "from datetime import datetime\n",
    "from ipykernel import get_connection_file\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# Set the notebook name for Weights and Biases tracking\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"embedding_model.ipynb\"\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()  # Ensure you are logged into Weights and Biases\n",
    "\n",
    "# Set the device to GPU if available, otherwise fallback to CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define the path to the notebook and the parent directory for imports\n",
    "notebook_path = os.path.abspath(\n",
    "    os.path.join(os.getcwd(), os.path.basename(get_connection_file()))\n",
    ")\n",
    "parent_dir = os.path.dirname(os.path.dirname(notebook_path))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Import utility functions and model utilities\n",
    "from utils.utils import *\n",
    "from utils.embedding_model_utils import *\n",
    "from utils.embedding_models.utils import *\n",
    "\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we decide whether to train the model on specific hyperparameters stored in config or to run a Weights and Biases sweep to locate the best hyperparameters as defined in `sweep_config.yaml`\n",
    "\n",
    "Set `run_sweep=True` to run the sweep using one of the sweep_config.yaml files and `False` to train the model on the defined config variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide whether to run a hyperparameter sweep or use a fixed configuration\n",
    "run_sweep = True\n",
    "sweep_config_file_path = \"sweep_config_ComplEx.yaml\"\n",
    "if run_sweep:\n",
    "    with open(sweep_config_file_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "else:\n",
    "    config = {\n",
    "        \"dataset_name\": \"FB15k_237\",\n",
    "        \"embedding_model_name\": \"TransE\",\n",
    "        \"task\": \"kg_completion\",\n",
    "        \"max_epochs\": 100000,\n",
    "        \"batch_size\": 512,\n",
    "        \"lr\": 0.075,\n",
    "        \"weight_decay\": 1e-07,\n",
    "        \"k\": [1, 3, 10],  # used for top-k evaluation\n",
    "        \"hidden_channels\": 512,\n",
    "        \"margin\": 0.5,\n",
    "        \"p_norm\": 2,\n",
    "        \"verbose\": True,\n",
    "        \"num_epochs_without_improvement_until_early_finish\": 1,\n",
    "        \"validate_after_this_many_epochs\": 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config: Dict) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Constructs and initializes the model based on the provided configuration.\n",
    "\n",
    "    Args:\n",
    "        config (Dict): Configuration dictionary containing model and training settings.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The initialized model ready for training.\n",
    "    \"\"\"\n",
    "    # Load dataset based on configuration\n",
    "    train_data, val_data, test_data, data_path = load_dataset(\n",
    "        config[\"dataset_name\"], parent_dir=parent_dir\n",
    "    )\n",
    "\n",
    "    # Determine feature dimensions if available\n",
    "    head_node_feature_dim = (\n",
    "        train_data.x.shape[1]\n",
    "        if hasattr(train_data, \"x\") and train_data.x is not None\n",
    "        else None\n",
    "    )\n",
    "    aux_dict = train_data.aux_dict if hasattr(train_data, \"aux_dict\") else None\n",
    "\n",
    "    config[\"num_nodes\"] = train_data.num_nodes\n",
    "    config[\"num_relations\"] = train_data.num_edge_types\n",
    "\n",
    "    # Create a dictionary of the required parameters\n",
    "    model_params = {\n",
    "        \"num_nodes\": config[\"num_nodes\"],\n",
    "        \"num_relations\": config[\"num_relations\"],\n",
    "        \"hidden_channels\": config[\"hidden_channels\"],\n",
    "        \"task\": config[\"task\"],\n",
    "        **({\"margin\": config[\"margin\"]} if \"margin\" in config else {}),\n",
    "        **({\"p_norm\": config[\"p_norm\"]} if \"p_norm\" in config else {}),\n",
    "    }\n",
    "\n",
    "    # Conditionally add optional parameters to both config and model_params\n",
    "    if head_node_feature_dim:\n",
    "        config[\"head_node_feature_dim\"] = head_node_feature_dim\n",
    "        model_params[\"head_node_feature_dim\"] = head_node_feature_dim\n",
    "    if aux_dict:\n",
    "        config[\"aux_dict\"] = aux_dict\n",
    "        model_params[\"aux_dict\"] = aux_dict\n",
    "\n",
    "    # Get the model class from the model name provided in config\n",
    "    model_class = get_embedding_model_class(config[\"embedding_model_name\"])\n",
    "\n",
    "    # Instantiate the model with the parameters\n",
    "    original_model = model_class(**model_params).to(device)\n",
    "\n",
    "    # Calculate batch size based on the number of GPUs available\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    batch_size_per_gpu = config[\"batch_size\"]\n",
    "    batch_size = (\n",
    "        batch_size_per_gpu * num_gpus if num_gpus > 0 else batch_size_per_gpu\n",
    "    )\n",
    "    num_workers = num_gpus if num_gpus > 0 else 1\n",
    "\n",
    "    # Helper function to prepare loader arguments\n",
    "    def prepare_loader_args(data, batch_size, num_workers):\n",
    "        \"\"\"Prepares and returns loader arguments based on the data and configuration.\"\"\"\n",
    "        loader_args = {\n",
    "            \"head_index\": data.edge_index[0],\n",
    "            \"rel_type\": data.edge_type,\n",
    "            \"tail_index\": data.edge_index[1],\n",
    "            \"batch_size\": batch_size,\n",
    "            \"shuffle\": True,\n",
    "            \"num_workers\": num_workers,\n",
    "        }\n",
    "        # Add extra features if available\n",
    "        if hasattr(data, \"x\"):\n",
    "            loader_args[\"x\"] = data.x\n",
    "        if hasattr(data, \"y\"):\n",
    "            loader_args[\"y\"] = data.y\n",
    "        return loader_args\n",
    "\n",
    "    # Prepare loader arguments for train, validation, and test datasets\n",
    "    train_loader_args = prepare_loader_args(\n",
    "        train_data, batch_size, num_workers\n",
    "    )\n",
    "    val_loader_args = prepare_loader_args(val_data, batch_size, num_workers)\n",
    "    test_loader_args = prepare_loader_args(test_data, batch_size, num_workers)\n",
    "\n",
    "    # Create data loaders for training, validation, and testing\n",
    "    train_loader = original_model.loader(**train_loader_args)\n",
    "    val_loader = original_model.loader(**val_loader_args)\n",
    "    test_loader = original_model.loader(**test_loader_args)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = optim.Adagrad(\n",
    "        original_model.parameters(),\n",
    "        lr=config[\"lr\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    # Use DataParallel for multi-GPU setups on single server\n",
    "    if num_gpus > 1:\n",
    "        model = torch.nn.DataParallel(original_model)\n",
    "    else:\n",
    "        model = original_model\n",
    "\n",
    "    # Attach data and loaders to the model for easy access\n",
    "    model.train_data = train_data\n",
    "    model.val_data = val_data\n",
    "    model.test_data = test_data\n",
    "    model.train_loader = train_loader\n",
    "    model.val_loader = val_loader\n",
    "    model.test_loader = test_loader\n",
    "    model.optimizer = optimizer\n",
    "    model.config = config\n",
    "    model.batch_size = batch_size  # this is different from config batch_size when using multiple GPUs\n",
    "    model.original_model = (\n",
    "        original_model  # Keep a reference to the original model\n",
    "    )\n",
    "\n",
    "    # Determine the path to save results and create directory if it does not exist\n",
    "    if \"save_path\" in config:\n",
    "        save_path = config[\"save_path\"]\n",
    "        if not os.path.exists(save_path):\n",
    "            raise ValueError(\n",
    "                f\"Directory {save_path} does not exist in which to save the trained embedding models. Please create it before saving.\"\n",
    "            )\n",
    "        save_path = osp.join(\n",
    "            save_path,\n",
    "            f\"{config['prefix']}_embedding_model\",\n",
    "        )\n",
    "    else:\n",
    "        save_path = osp.join(\n",
    "            parent_dir,\n",
    "            \"trained_embedding_models\",\n",
    "            f\"{config['prefix']}_embedding_model\",\n",
    "        )\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    model.save_path = save_path\n",
    "\n",
    "    # Save the model configuration\n",
    "    save_embedding_model_config(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: Module) -> float:\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch over the training dataset.\n",
    "\n",
    "    Args:\n",
    "        model (Module): The model to be trained, which includes the data loaders and optimizer.\n",
    "\n",
    "    Returns:\n",
    "        float: The average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = total_examples = 0  # Initialize loss and example counters\n",
    "\n",
    "    # Iterate over batches of data in the training loader\n",
    "    for batch in model.train_loader:\n",
    "        model.optimizer.zero_grad()\n",
    "\n",
    "        # Extract data\n",
    "        head_index, rel_type, tail_index = (\n",
    "            batch[\"head_index\"],\n",
    "            batch[\"rel_type\"],\n",
    "            batch[\"tail_index\"],\n",
    "        )\n",
    "        x = batch.get(\"x\", None)  # Optional feature matrix\n",
    "        y = batch.get(\"y\", None)  # Optional labels\n",
    "\n",
    "        # Move data\n",
    "        head_index, rel_type, tail_index = (\n",
    "            head_index.to(device),\n",
    "            rel_type.to(device),\n",
    "            tail_index.to(device),\n",
    "        )\n",
    "        if x is not None:\n",
    "            x = x.to(device)\n",
    "        if y is not None:\n",
    "            y = y.to(device)\n",
    "\n",
    "        # Compute the loss for positive samples\n",
    "        positive_loss = model.original_model.loss(\n",
    "            head_index, rel_type, tail_index, x, y\n",
    "        )\n",
    "\n",
    "        # Perform negative sampling to generate negative triples\n",
    "        neg_head_index, neg_rel_type, neg_tail_index = (\n",
    "            model.original_model.random_sample(\n",
    "                head_index,\n",
    "                rel_type,\n",
    "                tail_index,\n",
    "            )\n",
    "        )\n",
    "        neg_head_index, neg_rel_type, neg_tail_index = (\n",
    "            neg_head_index.to(device),\n",
    "            neg_rel_type.to(device),\n",
    "            neg_tail_index.to(device),\n",
    "        )\n",
    "\n",
    "        # Compute the loss for negative samples\n",
    "        negative_loss = model.original_model.loss(\n",
    "            neg_head_index,\n",
    "            neg_rel_type,\n",
    "            neg_tail_index,\n",
    "            x,\n",
    "            y,\n",
    "        )\n",
    "\n",
    "        # Calculate total loss and perform backpropagation\n",
    "        loss = positive_loss + negative_loss\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * head_index.size(0)\n",
    "        total_examples += (\n",
    "            2 * head_index.numel()\n",
    "        )  # Count each head_index twice (once for positive and once for negative)\n",
    "\n",
    "    # Compute average loss over all examples\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "def test_model(model: Module, val: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Tests the model on the validation or test dataset and returns performance metrics.\n",
    "\n",
    "    Args:\n",
    "        model (Module): The model to be tested.\n",
    "        val (bool): Flag indicating whether to test on the validation dataset (default is False).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing performance metrics based on the model's task.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = total_examples = 0\n",
    "    loader = model.val_loader if val else model.test_loader\n",
    "    all_metrics = []\n",
    "    only_relation_prediction = val  # during validation, only evaluate on relation prediction - on the test set do head, relation, and tail prediction\n",
    "\n",
    "    for batch in loader:\n",
    "        # Extract data from the batch\n",
    "        head_index, rel_type, tail_index = (\n",
    "            batch[\"head_index\"],\n",
    "            batch[\"rel_type\"],\n",
    "            batch[\"tail_index\"],\n",
    "        )\n",
    "        x = batch.get(\"x\", None)\n",
    "        y = batch.get(\"y\", None)\n",
    "\n",
    "        # Move data to the appropriate device\n",
    "        head_index, rel_type, tail_index = (\n",
    "            head_index.to(device),\n",
    "            rel_type.to(device),\n",
    "            tail_index.to(device),\n",
    "        )\n",
    "        if x is not None:\n",
    "            x = x.to(device)\n",
    "        if y is not None:\n",
    "            y = y.to(device)\n",
    "\n",
    "        # Compute loss for the batch\n",
    "        loss = model.original_model.loss(\n",
    "            head_index,\n",
    "            rel_type,\n",
    "            tail_index,\n",
    "            x,\n",
    "            y,\n",
    "            model.original_model.task,\n",
    "            model.original_model.aux_dict,\n",
    "        )\n",
    "        total_loss += float(loss) * head_index.numel()\n",
    "        total_examples += head_index.numel()\n",
    "\n",
    "        # Calculate metrics for the batch\n",
    "        metrics = model.original_model.test(\n",
    "            head_index=head_index,\n",
    "            rel_type=rel_type,\n",
    "            tail_index=tail_index,\n",
    "            x=x,\n",
    "            y=y,\n",
    "            batch_size=model.batch_size,\n",
    "            k=model.config.k,\n",
    "            task=model.original_model.task,\n",
    "            only_relation_prediction=only_relation_prediction,  # for kg_completion: during validation, only evaluate on relation prediction - on the test set do head, relation, and tail prediction\n",
    "        )\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "    # Aggregate metrics across all batches\n",
    "    if model.original_model.task in \"kg_completion\":\n",
    "        if only_relation_prediction:\n",
    "            relation_mean_ranks = []\n",
    "            relation_mrrs = []\n",
    "            relation_hits_at_ks = []\n",
    "            # Collect the metrics from each batch\n",
    "            for metrics in all_metrics:\n",
    "                relation_mean_rank, relation_mrr, relation_hits_at_k = metrics\n",
    "                relation_mean_ranks.append(relation_mean_rank)\n",
    "                relation_mrrs.append(relation_mrr)\n",
    "                relation_hits_at_ks.append(relation_hits_at_k)\n",
    "            # Aggregate the metrics across all batches\n",
    "            relation_mean_rank = sum(relation_mean_ranks) / len(all_metrics)\n",
    "            relation_mrr = sum(relation_mrrs) / len(all_metrics)\n",
    "\n",
    "            # Aggregate the hits@k metrics\n",
    "            all_k_values = set().union(\n",
    "                *(d.keys() for d in relation_hits_at_ks)\n",
    "            )\n",
    "            relation_hits_at_k = {\n",
    "                f\"relation_hits_at_{k}\": sum(\n",
    "                    d.get(k, 0) for d in relation_hits_at_ks\n",
    "                )\n",
    "                / len(all_metrics)\n",
    "                for k in all_k_values\n",
    "            }\n",
    "            # Create the performance_metrics dictionary\n",
    "            performance_metrics = {\n",
    "                \"loss\": total_loss / total_examples,\n",
    "                \"relation_mean_rank\": relation_mean_rank,\n",
    "                \"relation_mrr\": relation_mrr,\n",
    "                **relation_hits_at_k,\n",
    "            }\n",
    "        else:\n",
    "            head_mean_ranks = []\n",
    "            relation_mean_ranks = []\n",
    "            tail_mean_ranks = []\n",
    "            head_mrrs = []\n",
    "            relation_mrrs = []\n",
    "            tail_mrrs = []\n",
    "            head_hits_at_ks = []\n",
    "            relation_hits_at_ks = []\n",
    "            tail_hits_at_ks = []\n",
    "\n",
    "            # Collect the metrics from each batch\n",
    "            for metrics in all_metrics:\n",
    "                (\n",
    "                    head_mean_rank,\n",
    "                    relation_mean_rank,\n",
    "                    tail_mean_rank,\n",
    "                    head_mrr,\n",
    "                    relation_mrr,\n",
    "                    tail_mrr,\n",
    "                    head_hits_at_k,\n",
    "                    relation_hits_at_k,\n",
    "                    tail_hits_at_k,\n",
    "                ) = metrics\n",
    "                head_mean_ranks.append(head_mean_rank)\n",
    "                relation_mean_ranks.append(relation_mean_rank)\n",
    "                tail_mean_ranks.append(tail_mean_rank)\n",
    "                head_mrrs.append(head_mrr)\n",
    "                relation_mrrs.append(relation_mrr)\n",
    "                tail_mrrs.append(tail_mrr)\n",
    "                head_hits_at_ks.append(head_hits_at_k)\n",
    "                relation_hits_at_ks.append(relation_hits_at_k)\n",
    "                tail_hits_at_ks.append(tail_hits_at_k)\n",
    "\n",
    "            # Aggregate the metrics across all batches\n",
    "            head_mean_rank = sum(head_mean_ranks) / len(all_metrics)\n",
    "            relation_mean_rank = sum(relation_mean_ranks) / len(all_metrics)\n",
    "            tail_mean_rank = sum(tail_mean_ranks) / len(all_metrics)\n",
    "            head_mrr = sum(head_mrrs) / len(all_metrics)\n",
    "            relation_mrr = sum(relation_mrrs) / len(all_metrics)\n",
    "            tail_mrr = sum(tail_mrrs) / len(all_metrics)\n",
    "\n",
    "            # Aggregate the hits@k metrics\n",
    "            all_k_values = set().union(\n",
    "                *(d.keys() for d in relation_hits_at_ks)\n",
    "            )\n",
    "            head_hits_at_k = {\n",
    "                f\"head_hits_at_{k}\": sum(d.get(k, 0) for d in head_hits_at_ks)\n",
    "                / len(all_metrics)\n",
    "                for k in all_k_values\n",
    "            }\n",
    "            relation_hits_at_k = {\n",
    "                f\"relation_hits_at_{k}\": sum(\n",
    "                    d.get(k, 0) for d in relation_hits_at_ks\n",
    "                )\n",
    "                / len(all_metrics)\n",
    "                for k in all_k_values\n",
    "            }\n",
    "            tail_hits_at_k = {\n",
    "                f\"tail_hits_at_{k}\": sum(d.get(k, 0) for d in tail_hits_at_ks)\n",
    "                / len(all_metrics)\n",
    "                for k in all_k_values\n",
    "            }\n",
    "\n",
    "            # Create the performance_metrics dictionary\n",
    "            performance_metrics = {\n",
    "                \"loss\": total_loss / total_examples,\n",
    "                \"head_mean_rank\": head_mean_rank,\n",
    "                \"relation_mean_rank\": relation_mean_rank,\n",
    "                \"tail_mean_rank\": tail_mean_rank,\n",
    "                \"head_mrr\": head_mrr,\n",
    "                \"relation_mrr\": relation_mrr,\n",
    "                \"tail_mrr\": tail_mrr,\n",
    "                **head_hits_at_k,\n",
    "                **relation_hits_at_k,\n",
    "                **tail_hits_at_k,\n",
    "            }\n",
    "\n",
    "    elif model.original_model.task == \"node_classification\":\n",
    "        accuracy = sum(all_metrics) / len(all_metrics)\n",
    "        performance_metrics = {\n",
    "            \"loss\": total_loss / total_examples,\n",
    "            \"accuracy\": accuracy,\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"model task isn't valid: {model.original_model.task}\"\n",
    "        )\n",
    "\n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model on Validation Loss Improvement\n",
    "\n",
    "To ensure that the best model is saved in case the hyperparameter sweep is canceled, the model can be saved whenever there is an improvement in the validation loss. This can be achieved by monitoring the validation loss during training and saving the model whenever a new best validation loss is achieved. By doing so, you can retain the best model obtained during training even if the sweep is interrupted or canceled.\n",
    "\n",
    "### Main Function Overview\n",
    "\n",
    "The `main()` function serves as the core function for training the model and managing the training process. It initializes the training configuration, sets up the project environment using Weights and Biases for tracking, builds the model based on the provided configuration, and executes the training loop. Within `main()`, key functionalities include setting up the project, handling dataset-specific configurations, training the model, monitoring training progress, and saving the best model based on validation loss improvement. Additionally, it allows for early stopping based on increasing training loss and provides detailed logging of training metrics and test performance. `main()` orchestrates the training process and ensures the model is saved optimally during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    config: Optional[dict] = None,\n",
    "    num_epochs_without_improvement_until_early_finish: int = 5,\n",
    "    validate_after_this_many_epochs: int = 10,\n",
    "):\n",
    "    run_timestamp = datetime.now().strftime(\"%Y.%m.%d.%H.%M.%S\")\n",
    "\n",
    "    with wandb.init(\n",
    "        project=f\"ScoreMatchingDiffKG_Embedding\",\n",
    "        name=f\"{run_timestamp}_embedding_model_run\",\n",
    "        config=config if config else {},\n",
    "    ):\n",
    "        config = wandb.config\n",
    "        config[\"prefix\"] = generate_prefix(config, run_timestamp)\n",
    "        wandb.run.name = f\"{config['prefix']}_embedding_model_run\"\n",
    "\n",
    "        if (\n",
    "            wandb.config.task == \"node_classification\"\n",
    "            and wandb.config.dataset_name\n",
    "            not in [\n",
    "                \"Cora\",\n",
    "                \"Citeseer\",\n",
    "                \"Pubmed\",\n",
    "            ]\n",
    "        ) or (\n",
    "            wandb.config.task != \"node_classification\"\n",
    "            and wandb.config.dataset_name\n",
    "            in [\n",
    "                \"Cora\",\n",
    "                \"Citeseer\",\n",
    "                \"Pubmed\",\n",
    "            ]\n",
    "        ):\n",
    "            print(\n",
    "                f\"Skipping {wandb.config.task} on {wandb.config.dataset_name}\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        model = build_model(config)\n",
    "        wandb.watch(model)\n",
    "\n",
    "        num_epochs_without_improvement_until_early_finish = config.get(\n",
    "            \"num_epochs_without_improvement_until_early_finish\",\n",
    "            num_epochs_without_improvement_until_early_finish,\n",
    "        )\n",
    "        validate_after_this_many_epochs = config.get(\n",
    "            \"validate_after_this_many_epochs\", validate_after_this_many_epochs\n",
    "        )\n",
    "        best_metric_to_optimize = float(\n",
    "            \"inf\"\n",
    "        )  # either val mean rank or val accuracy later\n",
    "        metric_to_optimize = (\n",
    "            \"relation_mean_rank\"\n",
    "            if config[\"task\"] == \"kg_completion\"\n",
    "            else \"accuracy\"\n",
    "        )  # if config[\"task\"] == \"node_classification\"\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "        for epoch in range(config.max_epochs):\n",
    "            loss = train_model(model)\n",
    "\n",
    "            if config[\"verbose\"]:\n",
    "                print(f\"Epoch: {epoch:03d}, Train Loss: {loss:.10f}\")\n",
    "\n",
    "            train_metrics = {\"train_epoch\": epoch, \"train_loss\": loss}\n",
    "\n",
    "            if epoch % validate_after_this_many_epochs == 0 and epoch > 0:\n",
    "                val_metrics = test_model(model, val=True)\n",
    "                if val_metrics[metric_to_optimize] <= best_metric_to_optimize:\n",
    "                    best_metric_to_optimize = val_metrics[metric_to_optimize]\n",
    "                    save_trained_embedding_weights_and_performance(\n",
    "                        model, epoch + 1, val_metrics\n",
    "                    )\n",
    "                    epochs_without_improvement = 0\n",
    "                else:\n",
    "                    epochs_without_improvement += 1\n",
    "\n",
    "                if config[\"verbose\"]:\n",
    "                    metrics_info = \", \".join(\n",
    "                        [\n",
    "                            f'Val {key.replace(\"_\", \" \").title()}: {value:.10f}'\n",
    "                            for key, value in val_metrics.items()\n",
    "                            if isinstance(value, (int, float))\n",
    "                        ]\n",
    "                    )\n",
    "                    print(metrics_info)\n",
    "\n",
    "                if (\n",
    "                    epochs_without_improvement\n",
    "                    >= num_epochs_without_improvement_until_early_finish\n",
    "                ):\n",
    "                    print(\"Stopping early due to increasing training loss.\")\n",
    "                    break\n",
    "\n",
    "                val_metrics = {\n",
    "                    f\"val_{key}\": value for key, value in val_metrics.items()\n",
    "                }\n",
    "\n",
    "            wandb.log(\n",
    "                {**train_metrics, **val_metrics}\n",
    "                if \"val_metrics\" in locals()\n",
    "                else {**train_metrics}\n",
    "            )\n",
    "\n",
    "        model.load_state_dict(\n",
    "            torch.load(\n",
    "                osp.join(\n",
    "                    model.save_path,\n",
    "                    f\"{model.config['prefix']}_embedding_model_weights.pth\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        test_metrics = test_model(model)\n",
    "\n",
    "        if config[\"verbose\"]:\n",
    "            test_metrics_info = \", \".join(\n",
    "                [\n",
    "                    f'Test {key.replace(\"_\", \" \").title()}: {value:.4f}'\n",
    "                    for key, value in test_metrics.items()\n",
    "                    if isinstance(value, (int, float))\n",
    "                ]\n",
    "            )\n",
    "            print(test_metrics_info)\n",
    "\n",
    "        with open(\n",
    "            osp.join(\n",
    "                model.save_path,\n",
    "                f\"{model.config['prefix']}_embedding_model_performance.txt\",\n",
    "            ),\n",
    "            \"a\",\n",
    "        ) as file:\n",
    "            file.write(\"Test Metrics:\\n\")\n",
    "            for metric, value in test_metrics.items():\n",
    "                file.write(f\"{metric}: {value}\\n\")\n",
    "\n",
    "        test_metrics = {\n",
    "            f\"test_{key}\": value for key, value in test_metrics.items()\n",
    "        }\n",
    "        wandb.log({**test_metrics})\n",
    "        wandb.save(model.save_path, base_path=parent_dir)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: jr31bq1b\n",
      "Sweep URL: https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Embedding_Sweep_2024.05.17.17.43.53/sweeps/jr31bq1b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 83y0mcnz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset_name: FB15k_237\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_model_name: ComplEx\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_channels: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tk: [1, 3, 10]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0020062256919422847\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 2500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs_without_improvement_until_early_finish: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttask: kg_completion\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidate_after_this_many_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tverbose: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/blaineh2/ScoreMatchingDiffKG/embedding_model/wandb/run-20240517_174402-83y0mcnz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Embedding_Sweep_2024.05.17.17.43.53/runs/83y0mcnz' target=\"_blank\">2024.05.17.17.44.01_embedding_model_run</a></strong> to <a href='https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Embedding_Sweep_2024.05.17.17.43.53' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Embedding_Sweep_2024.05.17.17.43.53/sweeps/jr31bq1b' target=\"_blank\">https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Embedding_Sweep_2024.05.17.17.43.53/sweeps/jr31bq1b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Embedding_Sweep_2024.05.17.17.43.53' target=\"_blank\">https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Embedding_Sweep_2024.05.17.17.43.53</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Embedding_Sweep_2024.05.17.17.43.53/sweeps/jr31bq1b' target=\"_blank\">https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Embedding_Sweep_2024.05.17.17.43.53/sweeps/jr31bq1b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Embedding_Sweep_2024.05.17.17.43.53/runs/83y0mcnz' target=\"_blank\">https://wandb.ai/uiuc_idealab_2024/ScoreMatchingDiffKG_Embedding_Sweep_2024.05.17.17.43.53/runs/83y0mcnz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early due to increasing training loss.\n"
     ]
    }
   ],
   "source": [
    "def run_sweep_or_main(run_sweep: bool, config: Optional[dict] = None) -> None:\n",
    "    \"\"\"\n",
    "    Runs a hyperparameter sweep or the main training process based on the provided flag.\n",
    "\n",
    "    Args:\n",
    "        run_sweep (bool): Flag indicating whether to run a hyperparameter sweep.\n",
    "        config (Optional[dict]): Configuration dictionary for the main training process.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if run_sweep:\n",
    "        run_timestamp = datetime.now().strftime(\"%Y.%m.%d.%H.%M.%S\")\n",
    "        sweep_id = wandb.sweep(\n",
    "            project=f\"ScoreMatchingDiffKG_Embedding_Sweep_{run_timestamp}\",\n",
    "            sweep=config,\n",
    "        )\n",
    "\n",
    "        wandb.agent(sweep_id, function=main)\n",
    "    else:\n",
    "        model = main(config=config)\n",
    "\n",
    "\n",
    "run_sweep_or_main(run_sweep=run_sweep, config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
