{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: Blaine Hill\n",
    "\n",
    "In this notebook, we program out how to embed a KG such as FB15k_237 using a KG Embedding model such as RotatE. The weights are saved under at the top of the project directory under trained_embedding_models/embedding_model_weights.pth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from icecream import ic\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.nn import Module\n",
    "from datetime import datetime\n",
    "from ipykernel import get_connection_file\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# Set the notebook name for Weights and Biases tracking\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"embedding_model.ipynb\"\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()  # Ensure you are logged into Weights and Biases\n",
    "\n",
    "# Set the device to GPU if available, otherwise fallback to CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define the path to the notebook and the parent directory for imports\n",
    "notebook_path = os.path.abspath(\n",
    "    os.path.join(os.getcwd(), os.path.basename(get_connection_file()))\n",
    ")\n",
    "parent_dir = os.path.dirname(os.path.dirname(notebook_path))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Import utility functions and model utilities\n",
    "from utils.utils import *\n",
    "from utils.embedding_models.utils import *\n",
    "\n",
    "\n",
    "def set_seed(seed_value=123):\n",
    "    \"\"\"Set the seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we decide whether to train the model on specific hyperparameters stored in config or to run a Weights and Biases sweep to locate the best hyperparameters as defined in `sweep_config.yaml`\n",
    "\n",
    "Set `run_sweep=True` to run the sweep and `False` to train the model on the defined config variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide whether to run a hyperparameter sweep or use a fixed configuration\n",
    "run_sweep = True\n",
    "sweep_config_file_path = \"sweep_config.yaml\"\n",
    "if run_sweep:\n",
    "    with open(sweep_config_file_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "else:\n",
    "    config = {\n",
    "        \"dataset_name\": \"Cora\",\n",
    "        \"embedding_model_name\": \"ComplEx\",\n",
    "        \"task\": \"node_classification\",  # relation_prediction=link prediction\n",
    "        \"epochs\": 10000,\n",
    "        \"batch_size\": 512,\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 1e-6,\n",
    "        \"k\": 10,  # used for top-k evaluation\n",
    "        \"hidden_channels\": 512,\n",
    "        \"verbose\": True,\n",
    "        \"max_epochs_without_improvement\": 5,\n",
    "        \"validate_after_this_many_epochs\": 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config: Dict) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Constructs and initializes the model based on the provided configuration.\n",
    "\n",
    "    Args:\n",
    "        config (Dict): Configuration dictionary containing model and training settings.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The initialized model ready for training.\n",
    "    \"\"\"\n",
    "    # Load dataset based on configuration\n",
    "    train_data, val_data, test_data, data_path = load_dataset(\n",
    "        config[\"dataset_name\"], parent_dir=parent_dir, device=device\n",
    "    )\n",
    "\n",
    "    # Determine feature dimensions if available\n",
    "    head_node_feature_dim = (\n",
    "        train_data.x.shape[1]\n",
    "        if hasattr(train_data, \"x\") and train_data.x is not None\n",
    "        else None\n",
    "    )\n",
    "    aux_dict = train_data.aux_dict if hasattr(train_data, \"aux_dict\") else None\n",
    "\n",
    "    # Get the model class from the model name provided in config\n",
    "    model_class = get_model_class(config[\"embedding_model_name\"])\n",
    "    original_model = model_class(\n",
    "        num_nodes=train_data.num_nodes,\n",
    "        num_relations=train_data.num_edge_types,\n",
    "        hidden_channels=config[\"hidden_channels\"],\n",
    "        head_node_feature_dim=head_node_feature_dim,\n",
    "        task=config[\"task\"],\n",
    "        aux_dict=aux_dict,\n",
    "    ).to(device)\n",
    "\n",
    "    # Calculate batch size based on the number of GPUs available\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    batch_size_per_gpu = config[\"batch_size\"]\n",
    "    batch_size = (\n",
    "        batch_size_per_gpu * num_gpus if num_gpus > 0 else batch_size_per_gpu\n",
    "    )\n",
    "    num_workers = num_gpus if num_gpus > 0 else 1\n",
    "\n",
    "    # Helper function to prepare loader arguments\n",
    "    def prepare_loader_args(data, batch_size, num_workers):\n",
    "        \"\"\"Prepares and returns loader arguments based on the data and configuration.\"\"\"\n",
    "        loader_args = {\n",
    "            \"head_index\": data.edge_index[0],\n",
    "            \"rel_type\": data.edge_type,\n",
    "            \"tail_index\": data.edge_index[1],\n",
    "            \"batch_size\": batch_size,\n",
    "            \"shuffle\": True,\n",
    "            \"num_workers\": num_workers,\n",
    "        }\n",
    "        # Add extra features if available\n",
    "        if hasattr(data, \"x\"):\n",
    "            loader_args[\"x\"] = data.x\n",
    "        if hasattr(data, \"y\"):\n",
    "            loader_args[\"y\"] = data.y\n",
    "        return loader_args\n",
    "\n",
    "    # Prepare loader arguments for train, validation, and test datasets\n",
    "    train_loader_args = prepare_loader_args(\n",
    "        train_data, batch_size, num_workers\n",
    "    )\n",
    "    val_loader_args = prepare_loader_args(val_data, batch_size, num_workers)\n",
    "    test_loader_args = prepare_loader_args(test_data, batch_size, num_workers)\n",
    "\n",
    "    # Create data loaders for training, validation, and testing\n",
    "    train_loader = original_model.loader(**train_loader_args)\n",
    "    val_loader = original_model.loader(**val_loader_args)\n",
    "    test_loader = original_model.loader(**test_loader_args)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = optim.Adagrad(\n",
    "        original_model.parameters(),\n",
    "        lr=config[\"lr\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    # Use DataParallel for multi-GPU setups on single server\n",
    "    if num_gpus > 1:\n",
    "        model = torch.nn.DataParallel(original_model)\n",
    "    else:\n",
    "        model = original_model\n",
    "\n",
    "    # Attach data and loaders to the model for easy access\n",
    "    model.train_data = train_data\n",
    "    model.val_data = val_data\n",
    "    model.test_data = test_data\n",
    "    model.train_loader = train_loader\n",
    "    model.val_loader = val_loader\n",
    "    model.test_loader = test_loader\n",
    "    model.optimizer = optimizer\n",
    "    model.config = config\n",
    "    model.original_model = (\n",
    "        original_model  # Keep a reference to the original model\n",
    "    )\n",
    "\n",
    "    # Determine the path to save results and create directory if it does not exist\n",
    "    if \"save_path\" in config:\n",
    "        save_path = config[\"save_path\"]\n",
    "        if not os.path.exists(save_path):\n",
    "            raise ValueError(\n",
    "                f\"Directory {save_path} does not exist in which to save the trained embedding models. Please create it before saving.\"\n",
    "            )\n",
    "    else:\n",
    "        save_path = osp.join(\n",
    "            parent_dir,\n",
    "            \"trained_embedding_models\",\n",
    "            f\"{config['prefix']}_embedding_model\",\n",
    "        )\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "    model.save_path = save_path\n",
    "\n",
    "    # Save the model configuration\n",
    "    save_model_config(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: Module) -> float:\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch over the training dataset.\n",
    "\n",
    "    Args:\n",
    "        model (Module): The model to be trained, which includes the data loaders and optimizer.\n",
    "\n",
    "    Returns:\n",
    "        float: The average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = total_examples = 0  # Initialize loss and example counters\n",
    "\n",
    "    # Iterate over batches of data in the training loader\n",
    "    for batch in model.train_loader:\n",
    "        model.optimizer.zero_grad()\n",
    "\n",
    "        # Extract data\n",
    "        head_index, rel_type, tail_index = (\n",
    "            batch[\"head_index\"],\n",
    "            batch[\"rel_type\"],\n",
    "            batch[\"tail_index\"],\n",
    "        )\n",
    "        x = batch.get(\"x\", None)  # Optional feature matrix\n",
    "        y = batch.get(\"y\", None)  # Optional labels\n",
    "\n",
    "        # Move data\n",
    "        head_index, rel_type, tail_index = (\n",
    "            head_index.to(device),\n",
    "            rel_type.to(device),\n",
    "            tail_index.to(device),\n",
    "        )\n",
    "        if x is not None:\n",
    "            x = x.to(device)\n",
    "        if y is not None:\n",
    "            y = y.to(device)\n",
    "\n",
    "        # Compute the loss for positive samples\n",
    "        positive_loss = model.original_model.loss(\n",
    "            head_index, rel_type, tail_index, x, y\n",
    "        )\n",
    "\n",
    "        # Perform negative sampling to generate negative triples\n",
    "        neg_head_index, neg_rel_type, neg_tail_index = (\n",
    "            model.original_model.random_sample(\n",
    "                head_index,\n",
    "                rel_type,\n",
    "                tail_index,\n",
    "                model.original_model.task,\n",
    "                model.original_model.aux_dict,\n",
    "            )\n",
    "        )\n",
    "        neg_head_index, neg_rel_type, neg_tail_index = (\n",
    "            neg_head_index.to(device),\n",
    "            neg_rel_type.to(device),\n",
    "            neg_tail_index.to(device),\n",
    "        )\n",
    "\n",
    "        # Compute the loss for negative samples\n",
    "        negative_loss = model.original_model.loss(\n",
    "            neg_head_index,\n",
    "            neg_rel_type,\n",
    "            neg_tail_index,\n",
    "            x,\n",
    "            y,\n",
    "            model.original_model.task,\n",
    "            model.original_model.aux_dict,\n",
    "        )\n",
    "\n",
    "        # Calculate total loss and perform backpropagation\n",
    "        loss = positive_loss + negative_loss\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * head_index.size(0)\n",
    "        total_examples += (\n",
    "            2 * head_index.numel()\n",
    "        )  # Count each head_index twice (once for positive and once for negative)\n",
    "\n",
    "    # Compute average loss over all examples\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "def test_model(model: Module, val: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Tests the model on the validation or test dataset and returns performance metrics.\n",
    "\n",
    "    Args:\n",
    "        model (Module): The model to be tested.\n",
    "        val (bool): Flag indicating whether to test on the validation dataset (default is False).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing performance metrics based on the model's task.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = total_examples = 0\n",
    "    loader = model.val_loader if val else model.test_loader\n",
    "\n",
    "    for batch in loader:\n",
    "        # Extract data from the batch\n",
    "        head_index, rel_type, tail_index = (\n",
    "            batch[\"head_index\"],\n",
    "            batch[\"rel_type\"],\n",
    "            batch[\"tail_index\"],\n",
    "        )\n",
    "        x = batch.get(\"x\", None)\n",
    "        y = batch.get(\"y\", None)\n",
    "\n",
    "        # Move data to the appropriate device\n",
    "        head_index, rel_type, tail_index = (\n",
    "            head_index.to(device),\n",
    "            rel_type.to(device),\n",
    "            tail_index.to(device),\n",
    "        )\n",
    "        if x is not None:\n",
    "            x = x.to(device)\n",
    "        if y is not None:\n",
    "            y = y.to(device)\n",
    "\n",
    "        # Compute loss for the batch\n",
    "        loss = model.original_model.loss(\n",
    "            head_index,\n",
    "            rel_type,\n",
    "            tail_index,\n",
    "            x,\n",
    "            y,\n",
    "            model.original_model.task,\n",
    "            model.original_model.aux_dict,\n",
    "        )\n",
    "        total_loss += float(loss) * head_index.numel()\n",
    "        total_examples += head_index.numel()\n",
    "\n",
    "    # Calculate additional metrics based on the model's task\n",
    "    metrics = model.original_model.test(\n",
    "        head_index=head_index,\n",
    "        rel_type=rel_type,\n",
    "        tail_index=tail_index,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        batch_size=model.config.batch_size,\n",
    "        k=model.config.k,\n",
    "        task=model.original_model.task,\n",
    "    )\n",
    "\n",
    "    # Determine performance metrics based on the model's task\n",
    "    if model.original_model.task in [\n",
    "        \"relation_prediction\",\n",
    "        \"head_prediction\",\n",
    "        \"tail_prediction\",\n",
    "    ]:\n",
    "        mean_rank, mrr, hits_at_k = metrics\n",
    "        performance_metrics = {\n",
    "            \"loss\": total_loss / total_examples,\n",
    "            \"mean_rank\": mean_rank,\n",
    "            \"mrr\": mrr,\n",
    "            \"hits_at_k\": hits_at_k,\n",
    "        }\n",
    "    elif model.original_model.task == \"node_classification\":\n",
    "        accuracy = metrics\n",
    "        performance_metrics = {\n",
    "            \"loss\": total_loss / total_examples,\n",
    "            \"accuracy\": accuracy,\n",
    "        }\n",
    "\n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    config: Optional[dict] = None,\n",
    "    max_epochs_without_improvement: int = 5,\n",
    "    validate_after_this_many_epochs: int = 10,\n",
    "):\n",
    "    run_timestamp = datetime.now().strftime(\"%Y.%m.%d.%H.%M.%S\")\n",
    "\n",
    "    with wandb.init(\n",
    "        project=f\"ScoreMatchingDiffKG_Embedding\",\n",
    "        name=f\"{run_timestamp}_run\",\n",
    "        config=config if config else {},\n",
    "    ):\n",
    "        config = wandb.config\n",
    "        config[\"prefix\"] = (\n",
    "            f'{config[\"dataset_name\"]}_{config[\"embedding_model_name\"]}_{run_timestamp}_{generate_unique_string(config)}'\n",
    "        )\n",
    "        dataset_name = wandb.config.dataset_name\n",
    "        task = wandb.config.task\n",
    "\n",
    "        if (\n",
    "            task == \"node_classification\"\n",
    "            and dataset_name\n",
    "            not in [\n",
    "                \"Cora\",\n",
    "                \"Citeseer\",\n",
    "                \"Pubmed\",\n",
    "            ]\n",
    "        ) or (\n",
    "            task != \"node_classification\"\n",
    "            and dataset_name\n",
    "            in [\n",
    "                \"Cora\",\n",
    "                \"Citeseer\",\n",
    "                \"Pubmed\",\n",
    "            ]\n",
    "        ):\n",
    "            print(f\"Skipping {task} on {dataset_name}\")\n",
    "            return\n",
    "\n",
    "        wandb.run.name = f\"{config['prefix']}_run\"\n",
    "\n",
    "        model = build_model(config)\n",
    "        wandb.watch(model)\n",
    "\n",
    "        max_epochs_without_improvement = config.get(\n",
    "            \"max_epochs_without_improvement\", max_epochs_without_improvement\n",
    "        )\n",
    "        validate_after_this_many_epochs = config.get(\n",
    "            \"validate_after_this_many_epochs\", validate_after_this_many_epochs\n",
    "        )\n",
    "        best_train_loss = best_val_loss = float(\"inf\")\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            loss = train_model(model)\n",
    "\n",
    "            if config[\"verbose\"]:\n",
    "                print(f\"Epoch: {epoch:03d}, Train Loss: {loss:.10f}\")\n",
    "\n",
    "            if loss <= best_train_loss:\n",
    "                best_train_loss = loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "                if (\n",
    "                    epochs_without_improvement\n",
    "                    >= max_epochs_without_improvement\n",
    "                ):\n",
    "                    print(\"Stopping early due to increasing training loss.\")\n",
    "                    break\n",
    "\n",
    "            train_metrics = {\"train_epoch\": epoch, \"train_loss\": loss}\n",
    "\n",
    "            if epoch % validate_after_this_many_epochs == 0 and epoch > 0:\n",
    "                val_metrics = test_model(model, val=True)\n",
    "\n",
    "                if config[\"verbose\"]:\n",
    "                    metrics_info = \", \".join(\n",
    "                        [\n",
    "                            f'Val {key.replace(\"_\", \" \").title()}: {value:.10f}'\n",
    "                            for key, value in val_metrics.items()\n",
    "                            if isinstance(value, (int, float))\n",
    "                        ]\n",
    "                    )\n",
    "                    print(metrics_info)\n",
    "\n",
    "                if val_metrics[\"loss\"] < best_val_loss:\n",
    "                    best_val_loss = val_metrics[\"loss\"]\n",
    "                    save_trained_embedding_model_and_config(\n",
    "                        model, epoch, val_metrics\n",
    "                    )\n",
    "\n",
    "                val_metrics = {\n",
    "                    f\"val_{key}\": value for key, value in val_metrics.items()\n",
    "                }\n",
    "\n",
    "            wandb.log(\n",
    "                {**train_metrics, **val_metrics}\n",
    "                if \"val_metrics\" in locals()\n",
    "                else {**train_metrics}\n",
    "            )\n",
    "\n",
    "        model.load_state_dict(\n",
    "            torch.load(\n",
    "                osp.join(\n",
    "                    model.save_path,\n",
    "                    f\"{model.config['prefix']}_embedding_model_weights.pth\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        test_metrics = test_model(model)\n",
    "\n",
    "        if config[\"verbose\"]:\n",
    "            test_metrics_info = \", \".join(\n",
    "                [\n",
    "                    f'Test {key.replace(\"_\", \" \").title()}: {value:.4f}'\n",
    "                    for key, value in test_metrics.items()\n",
    "                    if isinstance(value, (int, float))\n",
    "                ]\n",
    "            )\n",
    "            print(test_metrics_info)\n",
    "\n",
    "        with open(\n",
    "            osp.join(\n",
    "                model.save_path,\n",
    "                f\"{model.config['prefix']}_embedding_model_performance.txt\",\n",
    "            ),\n",
    "            \"a\",\n",
    "        ) as file:\n",
    "            file.write(\"Test Metrics:\\n\")\n",
    "            for metric, value in test_metrics.items():\n",
    "                file.write(f\"{metric}: {value}\\n\")\n",
    "\n",
    "        test_metrics = {\n",
    "            f\"test_{key}\": value for key, value in test_metrics.items()\n",
    "        }\n",
    "        wandb.log({**test_metrics})\n",
    "        wandb.save(model.save_path)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sweep_or_main(run_sweep: bool, config: Optional[dict] = None) -> None:\n",
    "    \"\"\"\n",
    "    Runs a hyperparameter sweep or the main training process based on the provided flag.\n",
    "\n",
    "    Args:\n",
    "        run_sweep (bool): Flag indicating whether to run a hyperparameter sweep.\n",
    "        config (Optional[dict]): Configuration dictionary for the main training process.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if run_sweep:\n",
    "\n",
    "        sweep_id = wandb.sweep(\n",
    "            project=f\"ScoreMatchingDiffKG_Embedding_Sweep\", sweep=config\n",
    "        )\n",
    "\n",
    "        wandb.agent(sweep_id, function=main)\n",
    "    else:\n",
    "        model = main(config=config)\n",
    "\n",
    "\n",
    "run_sweep_or_main(run_sweep=run_sweep, config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
